---
{
    "name": "NEFER",
    "aliases": [],
    "year": 2023,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "Prophesee Gen4"
    ],
    "other_sensors": [
        "GoPro Hero+"
    ],
    "category": "Human-centric Recordings",
    "tags": [
        "Emotion Recognition",
        "Head Pose Estimation"
    ],
    "description": "Facial Expression Recognition",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": true,
        "distribution_methods": [
            "Google Drive"
        ],
        "file_formats": [
            "Binary"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Google Drive",
                "url": "https://drive.google.com/drive/folders/1OAnM8B88XX_TqJj2XF61w1nA-nWNV4YC?usp=sharing",
                "format": "Binary",
                "available": true
            }
        ],
        "size_gb": 8.5,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "Neuromorphic Event-based Facial Expression Recognition",
        "doi": "10.1109/CVPRW59228.2023.00432",
        "authors": [
            "Lorenzo Berlincioni",
            "Luca Cultrera",
            "Chiara Albisani",
            "Lisa Cresti",
            "Andrea Leonardo",
            "Sara Picchioni",
            "Federico Becattini",
            "Alberto Del Bimbo"
        ],
        "abstract": "Recently, event cameras have shown large applicability in several computer vision fields especially concerning tasks that require high temporal resolution. In this work, we investigate the usage of such kind of data for emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based Facial Expression Recognition. NEFER is composed of paired RGB and event videos representing human faces labeled with the respective emotions and also annotated with face bounding boxes and facial landmarks. We detail the data acquisition process as well as providing a baseline method for RGB and event data. The collected data captures subtle micro-expressions, which are hard to spot with RGB data, yet emerge in the event domain. We report a double recognition accuracy for the event-based approach, proving the effectiveness of a neuromorphic approach for analyzing fast and hardly detectable expressions and the emotions they conceal.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 22,
            "updated": "2025-06-16T11:44:35.737484"
        },
        {
            "source": "scholar",
            "count": 32,
            "updated": "2025-06-16T11:44:35.602275"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2304.06351"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/10208675"
        },
        {
            "type": "github_page",
            "url": "https://github.com/miccunifi/nefer"
        }
    ],
    "full_name": "Neuromorphic Event-based Facial Expression Recognition",
    "additional_metadata": {
        "num_recordings": "609",
        "num_subjects": "29",
        "num_classes": "7",
        "stereo": false
    },
    "bibtex": {
        "pages": "4109--4119",
        "year": 2023,
        "month": "jun",
        "author": "Berlincioni, Lorenzo and Cultrera, Luca and Albisani, Chiara and Cresti, Lisa and Leonardo, Andrea and Picchioni, Sara and Becattini, Federico and Del Bimbo, Alberto",
        "publisher": "IEEE",
        "booktitle": "2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})",
        "urldate": "2024-02-28",
        "language": "en",
        "doi": "10.1109/CVPRW59228.2023.00432",
        "url": "https://ieeexplore.ieee.org/document/10208675/",
        "isbn": "9798350302493",
        "title": "Neuromorphic {Event}-based {Facial} {Expression} {Recognition}",
        "address": "Vancouver, BC, Canada",
        "type": "inproceedings",
        "key": "berlincioni_neuromorphic_2023"
    },
    "referenced_papers": [
        {
            "doi": "10.1016/j.dsp.2020.102809",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2019.2941978",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC19947.2020.9063149",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCSVT.2022.3189480",
            "source": "crossref"
        },
        {
            "doi": "10.1145/2993148.2997632",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2015.510",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV45572.2020.9093607",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW53098.2021.00150",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ESSCIRC.2016.7598232",
            "source": "crossref"
        },
        {
            "doi": "10.1117/12.2520589",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TAFFC.2016.2573832",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.01256",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2021.3060707",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-01252-6_14",
            "source": "crossref"
        },
        {
            "doi": "10.3390/s20247079",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2021.03.019",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2019.2919301",
            "source": "crossref"
        },
        {
            "doi": "10.1109/AICAS48895.2020.9073789",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.690",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JPROC.2014.2346153",
            "source": "crossref"
        },
        {
            "doi": "10.1049/ic.2009.0244",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA40945.2020.9197341",
            "source": "crossref"
        },
        {
            "doi": "10.1109/3DV57658.2022.00023",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2515606",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.116",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3469877.3490621",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TITS.2020.3022921",
            "source": "crossref"
        },
        {
            "doi": "10.1109/FG.2018.00020",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCVW54120.2021.00103",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00097",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ITSC.2006.1706816",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00215",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00587",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-642-33712-3_49",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TII.2022.3195063",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR46437.2021.00490",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.01280",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA46639.2022.9811805",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW53098.2021.00144",
            "source": "crossref"
        },
        {
            "doi": "10.18178/joig.9.1.15-19",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW53098.2021.00151",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICPR48806.2021.9412991",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52688.2022.00860",
            "source": "crossref"
        },
        {
            "doi": "10.3390/s22041524",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00484",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-018-1097-z",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2021.702765",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR46437.2021.00768",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.527",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-012-0564-1",
            "source": "crossref"
        },
        {
            "doi": "10.3390/s22155687",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-01240-3_47",
            "source": "crossref"
        },
        {
            "title": "Universal facial expressions in emotion",
            "source": "crossref"
        },
        {
            "title": "Hyperextended light-face: A facial attribute analysis framework",
            "source": "crossref"
        },
        {
            "title": "ESIM: an open event camera simulator",
            "source": "crossref"
        },
        {
            "title": "Event camera simulator design for modeling attention-based inference architectures",
            "source": "crossref"
        },
        {
            "title": "Temporal activity detection in untrimmed videos with recurrent neural networks",
            "source": "crossref"
        },
        {
            "title": "Xception: Deep learning with depthwise separable convolutions",
            "source": "crossref"
        },
        {
            "title": "Deep facial expression recognition: A survey",
            "source": "crossref"
        },
        {
            "title": "A comprehensive analysis of deep learning based representation for face recognition",
            "source": "crossref"
        },
        {
            "title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments",
            "source": "crossref"
        },
        {
            "title": "Learning face representation from scratch",
            "source": "crossref"
        },
        {
            "title": "A discriminative feature learning approach for deep face recognition",
            "source": "crossref"
        },
        {
            "title": "Masked face recognition dataset and application",
            "source": "crossref"
        },
        {
            "title": "Casme database: A dataset of spontaneous micro&#x2013;expressions collected from neutralized faces",
            "source": "crossref"
        },
        {
            "title": "Ms-celeb-1m: A dataset and benchmark for large-scale face recognition",
            "source": "crossref"
        },
        {
            "title": "Event camera simulator improvements via characterized parameters",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

- Contains 609 recordings from 29 subjects
- Recordings are labelled with one of 7 different emotions
