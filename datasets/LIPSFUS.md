---
{
    "name": "LIPSFUS",
    "aliases": [],
    "year": 2023,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DAVIS346",
        "Dynamic Audio Sensor",
        "DVS128"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "Lip-reading"
    ],
    "task": "Lip Reading",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": false,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [
            "aedat"
        ],
        "availability_comment": "AEDAT files contain merged audio and video streams. No dataset link in Github Repository.",
        "dataset_links": []
    },
    "paper": {
        "title": "LIPSFUS: A neuromorphic dataset for audio-visual sensory fusion of lip reading",
        "doi": "10.1109/ISCAS46773.2023.10181685",
        "authors": [
            "A. Rios-Navarro",
            "E. Pi\u00f1ero-Fuentes",
            "S. Canas-Moreno",
            "A. Javed",
            "J. Harkin",
            "A. Linares-Barranco"
        ],
        "abstract": "This paper presents a sensory fusion neuromorphic dataset collected with precise temporal synchronization using a set of Address-Event-Representation sensors and tools. The target application is the lip reading of several keywords for different machine learning applications, such as digits, robotic commands, and auxiliary rich phonetic short words. The dataset is enlarged with a spiking version of an audio-visual lip reading dataset collected with frame-based cameras. LIPSFUS is publicly available and it has been validated with a deep learning architecture for audio and visual classification. It is intended for sensory fusion architectures based on both artificial and spiking neural network algorithms.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 1,
            "updated": "2025-06-24T05:59:21.071890"
        },
        {
            "source": "scholar",
            "count": 4,
            "updated": "2025-06-24T05:59:19.134996"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2304.01080"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/10181685"
        },
        {
            "type": "github_page",
            "url": "https://github.com/RTC-research-group/LIPSFUS-Event-driven-dataset/"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_subjectsL": "22",
        "stereo": true
    },
    "bibtex": {
        "pages": "1--5",
        "year": 2023,
        "month": "may",
        "author": "Rios-Navarro, A. and Pi\u00f1ero-Fuentes, E. and Canas-Moreno, S. and Javed, A. and Harkin, J. and Linares-Barranco, A.",
        "publisher": "IEEE",
        "booktitle": "2023 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})",
        "urldate": "2024-04-13",
        "language": "en",
        "doi": "10.1109/ISCAS46773.2023.10181685",
        "url": "https://ieeexplore.ieee.org/document/10181685/",
        "shorttitle": "{LIPSFUS}",
        "isbn": "978-1-66545-109-3",
        "copyright": "https://doi.org/10.15223/policy-029",
        "title": "{LIPSFUS}: {A} neuromorphic dataset for audio-visual sensory fusion of lip reading",
        "address": "Monterey, CA, USA",
        "type": "inproceedings",
        "key": "rios-navarro_lipsfus_2023"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/TITS.2006.888597",
            "source": "crossref"
        },
        {
            "doi": "10.1002/rob.21535",
            "source": "crossref"
        },
        {
            "doi": "10.1109/iemtronics55184.2022.9795815",
            "source": "crossref"
        },
        {
            "doi": "10.1109/5.58356",
            "source": "crossref"
        },
        {
            "doi": "10.1016/S0960-9822(03)00445-7",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neuropsychologia.2006.01.001",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2016.2583223",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2012.2230553",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neucom.2020.12.062",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TBCAS.2017.2717341",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2006.1693319",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2007.378616",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-54184-6_6",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.367",
            "source": "crossref"
        },
        {
            "title": "A smart healthcare recommendation system for multidisciplinary diabetes patients with data fusion based on deep ensemble learning",
            "source": "crossref"
        },
        {
            "title": "Multisensor data fusion: A review of the state-of-the-art",
            "source": "crossref"
        },
        {
            "title": "Multisensory integration: Strategies for synchronization",
            "source": "crossref"
        },
        {
            "title": "Multisensory integration: Maintaining the perception of synchrony",
            "source": "crossref"
        },
        {
            "title": "Temporal window of integration in auditory-visual speech perception",
            "source": "crossref"
        },
        {
            "title": "Audiovisual temporal fusion in 6-month-old infants",
            "source": "crossref"
        },
        {
            "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
            "source": "crossref"
        },
        {
            "title": "Wide feedforward or recurrent neural networks of any architecture are gaussian processes",
            "source": "crossref"
        },
        {
            "title": "Lip reading in profile",
            "source": "crossref"
        },
        {
            "title": "Ibm watson studio",
            "source": "crossref"
        },
        {
            "title": "Esim: an open event camera simulator",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

The dataset contains recordings from 22 subjects with ages from 6 to 60, both genders and 5 different nationalities. These files are in the folder "NosiyRoom". These files can be played for visualization in jAER, or they can be loaded into MATLAB with the attached sample scripts. In the folder We are now working in several SNN / CNN trainings for evaluating the goodness of this dataset, so we will come back here with more digested .aedat files once the work is finished.

We have repeated the experiment in a quiter room, designed for BCI recordings, prepared to isolate the person from external noise like building ventilation, people, traffic, etc. In this case we have recorded 21 people with ages from 25 to 60 where most of them are from Ireland.
