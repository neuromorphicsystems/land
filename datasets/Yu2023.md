---
{
    "name": "Yu2023",
    "aliases": [],
    "year": 2023,
    "modality": [
        "Audio",
        "Vision"
    ],
    "sensors": [
        "ESIM"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "Lip-reading"
    ],
    "task": "Lip Reading",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": false,
        "has_simulated_data": true,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [
            "None"
        ],
        "file_formats": [],
        "availability_comment": "",
        "dataset_links": []
    },
    "paper": {
        "title": "Multimodal Learning of Audio-Visual Speech Recognition with Liquid State Machine",
        "doi": "10.1007/978-981-99-1645-0_46",
        "authors": [
            "Xuhu Yu",
            "Lei Wang",
            "Changhao Chen",
            "Junbo Tie",
            "Shasha Guo"
        ],
        "abstract": "Audio-visual speech recognition is to solve the multimodal lip-reading task using audio and visual information, which is an important way to improve the performance of speech recognition in noisy conditions. Deep learning methods have achieved promising results in this regard. However, these methods have complex network architecture and are computationally intensive. Recently, Spiking Neural Networks (SNNs) have attracted attention due to their being event-driven and can enable low-power computing. SNNs can capture richer motion information and have been successful in work such as gesture recognition. But it has not been widely used in lipreading tasks. Liquid State Machines (LSMs) have been recognized in SNNs due to their low training costs and are well suited for spatiotemporal sequence problems of event streams. Multimodal lipreading based on Dynamic Vision Sensors (DVS) is also such a problem. Hence, we propose a soft fusion framework with LSM. The framework fuses visual and audio information to achieve the effect of higher reliability lip recognition. On the well-known public LRW dataset, our fusion network achieves a recognition accuracy of 86.8%. Compared with single modality recognition, the accuracy of the fusion method is improved by 5% to 6%. In addition, we add extra noise to the raw data, and the experimental results show that the fusion model outperforms the audio-only model significantly, proving the robustness of our model.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 2,
            "updated": "2024-10-26 12:52:42.432492"
        },
        {
            "source": "scholar",
            "count": 5,
            "updated": "2025-06-22T13:02:56.852357"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://link.springer.com/chapter/10.1007/978-981-99-1645-0_46"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "source_dataset": "LRW"
    },
    "bibtex": {
        "pages": "552--563",
        "note": "Series Title: Communications in Computer and Information Science",
        "year": 2023,
        "editor": "Tanveer, Mohammad and Agarwal, Sonali and Ozawa, Seiichi and Ekbal, Asif and Jatowt, Adam",
        "author": "Yu, Xuhu and Wang, Lei and Chen, Changhao and Tie, Junbo and Guo, Shasha",
        "publisher": "Springer Nature Singapore",
        "booktitle": "Neural {Information} {Processing}",
        "urldate": "2024-12-14",
        "language": "en",
        "doi": "10.1007/978-981-99-1645-0_46",
        "url": "https://link.springer.com/10.1007/978-981-99-1645-0_46",
        "isbn": "978-981-9916-44-3 978-981-9916-45-0",
        "volume": "1793",
        "title": "Multimodal {Learning} of {Audio}-{Visual} {Speech} {Recognition} with {Liquid} {State} {Machine}",
        "address": "Singapore",
        "type": "inproceedings",
        "key": "tanveer_multimodal_2023"
    }
}
---

### Dataset Structure

- Based on the LRW dataset
