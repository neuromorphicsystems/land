---
{
    "name": "Chen2019",
    "aliases": [],
    "year": 2019,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DAVIS240"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "Pedestrian Detection"
    ],
    "task": "Pedestrian detection",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [
            "None"
        ],
        "file_formats": [],
        "availability_comment": "Dataset available on request",
        "dataset_links": []
    },
    "paper": {
        "title": "Multi-Cue Event Information Fusion for Pedestrian Detection With Neuromorphic Vision Sensors",
        "doi": "10.3389/fnbot.2019.00010",
        "authors": [
            "Guang Chen",
            "Hu Cao",
            "Canbo Ye",
            "Zhenyan Zhang",
            "Xingbo Liu",
            "Xuhui Mo",
            "Zhongnan Qu",
            "J\u00f6rg Conradt",
            "Florian R\u00f6hrbein",
            "Alois Knoll"
        ],
        "abstract": "Neuromorphic vision sensors are bio-inspired cameras that naturally capture the dynamics of a scene with ultra-low latency, filtering out redundant information with low power consumption. Few works are addressing the object detection with this sensor. In this work, we propose to develop pedestrian detectors that unlock the potential of the event data by leveraging multi-cue information and different fusion strategies. To make the best out of the event data, we introduce three different event-stream encoding methods based on Frequency, Surface of Active Event (SAE) and Leaky Integrate-and-Fire (LIF). We further integrate them into the state-of-the-art neural network architectures with two fusion approaches: the channel-level fusion of the raw feature space and decision-level fusion with the probability assignments. We present a qualitative and quantitative explanation why different encoding methods are chosen to evaluate the pedestrian detection and which method performs the best. We demonstrate the advantages of the decision-level fusion via leveraging multi-cue event information and show that our approach performs well on a self-annotated event-based pedestrian dataset with 8,736 event frames. This work paves the way of more fascinating perception applications with neuromorphic vision sensors.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 28,
            "updated": "2025-07-08T07:16:07.474277"
        },
        {
            "source": "scholar",
            "count": 48,
            "updated": "2025-07-08T07:16:07.225206"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2019.00010/full"
        },
        {
            "type": "github_page",
            "url": "https://github.com/colinshane/DVS-multi-cue-pedestrian-detection"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_recordings": "4",
        "total_duration": "458.2 seconds",
        "stereo": false
    },
    "bibtex": {
        "pages": "10",
        "year": 2019,
        "month": "apr",
        "author": "Chen, Guang and Cao, Hu and Ye, Canbo and Zhang, Zhenyan and Liu, Xingbo and Mo, Xuhui and Qu, Zhongnan and Conradt, J\u00f6rg and R\u00f6hrbein, Florian and Knoll, Alois",
        "journal": "Frontiers in Neurorobotics",
        "urldate": "2024-12-07",
        "language": "en",
        "doi": "10.3389/fnbot.2019.00010",
        "url": "https://www.frontiersin.org/article/10.3389/fnbot.2019.00010/full",
        "issn": "1662-5218",
        "volume": "13",
        "title": "Multi-{Cue} {Event} {Information} {Fusion} for {Pedestrian} {Detection} {With} {Neuromorphic} {Vision} {Sensors}",
        "type": "article",
        "key": "chen_multi-cue_2019"
    },
    "referenced_papers": [
        {
            "doi": "10.1007/s00422-006-0068-6",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TITS.2015.2479925",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2014.2300479",
            "source": "crossref"
        },
        {
            "doi": "10.5244/C.23.91",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIP.2011.2142006",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCYB.2016.2593940",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-10584-0_23",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV.2016.7477574",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.106",
            "source": "crossref"
        },
        {
            "doi": "10.5244/C.31.33",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS.2014.6943141",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA.2016.7487370",
            "source": "crossref"
        },
        {
            "title": "A 240\u2013180 10mw 12us latency sparse-output vision sensor for mobile applications",
            "source": "crossref"
        },
        {
            "title": "A review of the integrate-and-fire neuron model: I. homogeneous synaptic input",
            "source": "crossref"
        },
        {
            "title": "Event based convolutional networks for object detection in neuromorphic cameras",
            "source": "crossref"
        },
        {
            "title": "Multiple sensor fusion and classification for moving object detection and tracking",
            "source": "crossref"
        },
        {
            "title": "Pseudo-labels for supervised learning on dynamic vision sensor data, applied to object detection under ego-motion",
            "source": "crossref"
        },
        {
            "title": "Multi-view 3d object detection network for autonomous driving",
            "source": "crossref"
        },
        {
            "title": "Fast feature pyramids for object detection",
            "source": "crossref"
        },
        {
            "title": "Integral channel feature",
            "source": "crossref"
        },
        {
            "title": "A multilevel mixture-of-experts framework for pedestrian classification",
            "source": "crossref"
        },
        {
            "title": "On-board object detection: multicue, multimodal, and multiview random forest of local experts",
            "source": "crossref"
        },
        {
            "title": "Learning rich features from RGB-D images for object detection and segmentation",
            "source": "crossref"
        },
        {
            "title": "Hots: a hierarchy of event-based time-surfaces for pattern recognition",
            "source": "crossref"
        },
        {
            "title": "Dynamic belief fusion for object detection",
            "source": "crossref"
        },
        {
            "title": "A 128 \u00d7 128 120 db 15s latency asynchronous temporal contrast vision sensor",
            "source": "crossref"
        },
        {
            "title": "Feature pyramid networks for object detection",
            "source": "crossref"
        },
        {
            "title": "Combined frame-and event-based detection and tracking",
            "source": "crossref"
        },
        {
            "title": "Ssd: single shot multibox detector",
            "source": "crossref"
        },
        {
            "title": "Fast event-based corner detection",
            "source": "crossref"
        },
        {
            "title": "A qvga 143 db dynamic range frame-free pwm image sensor with lossless pixel-level video compression and time-domain cds",
            "source": "crossref"
        },
        {
            "title": "Pedestrian detection combining RGB and dense LIDAR data",
            "source": "crossref"
        },
        {
            "title": "DART: distribution Aware Retinal Transform for event-based cameras",
            "source": "crossref"
        },
        {
            "title": "You only look once: unified, real-time object detection",
            "source": "crossref"
        },
        {
            "title": "Yolov3: an incremental improvement",
            "source": "crossref"
        },
        {
            "title": "Faster r-cnn: towards real-time object detection with region proposal networks",
            "source": "crossref"
        },
        {
            "title": "Fusing lidar and images for pedestrian detection using convolutional neural networks",
            "source": "crossref"
        }
    ]
}
---


### Dataset Structure 


- Contains 4 recordings
- Contains a total of 458.2 seconds