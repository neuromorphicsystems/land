---
{
    "name": "FES",
    "aliases": [],
    "year": 2024,
    "modalities": [
        "Vision"
    ],
    "sensors": [
        "Prophesee Gen3"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Face Detection"
    ],
    "description": "Face Detection",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [
            "Google Drive"
        ],
        "file_formats": [
            "Binary"
        ],
        "availability_comment": "Form submission required to access dataset",
        "dataset_links": [],
        "size_gb": 705.8,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "Faces in Event Streams (FES): An Annotated Face Dataset for Event Cameras",
        "doi": "10.3390/s24051409",
        "authors": [
            "Ulzhan Bissarinova",
            "Tomiris Rakhimzhanova",
            "Daulet Kenzhebalin",
            "Huseyin Atakan Varol"
        ],
        "abstract": "The use of event-based cameras in computer vision is a growing research direction. However, despite the existing research on face detection using the event camera, a substantial gap persists in the availability of a large dataset featuring annotations for faces and facial landmarks on event streams, thus hampering the development of applications in this direction. In this work, we address this issue by publishing the first large and varied dataset (Faces in Event Streams) with a duration of 689 min for face and facial landmark detection in direct event-based camera outputs. In addition, this article presents 12 models trained on our dataset to predict bounding box and facial landmark coordinates with an mAP50 score of more than 90\\%. We also performed a demonstration of real-time detection with an event-based camera using our models.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 5,
            "updated": "2025-06-24T06:19:05.127005"
        },
        {
            "source": "scholar",
            "count": 8,
            "updated": "2025-06-24T06:19:04.237690"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://www.mdpi.com/1424-8220/24/5/1409"
        },
        {
            "type": "github_page",
            "url": "https://github.com/IS2AI/faces-in-event-streams"
        }
    ],
    "full_name": "Faces in Event Streams (FES)",
    "additional_metadata": {
        "num_recordings": "1.6 million",
        "num_subjects": "73",
        "num_female": "31",
        "num_male": "42",
        "size_uncompressed": "629.2 Gb",
        "total_duration": "689 min"
    },
    "bibtex": {
        "pages": "1409",
        "year": 2024,
        "month": "feb",
        "author": "Bissarinova, Ulzhan and Rakhimzhanova, Tomiris and Kenzhebalin, Daulet and Varol, Huseyin Atakan",
        "journal": "Sensors",
        "urldate": "2024-03-11",
        "number": "5",
        "language": "en",
        "doi": "10.3390/s24051409",
        "url": "https://www.mdpi.com/1424-8220/24/5/1409",
        "shorttitle": "Faces in {Event} {Streams} ({FES})",
        "issn": "1424-8220",
        "volume": "24",
        "title": "Faces in {Event} {Streams} ({FES}): {An} {Annotated} {Face} {Dataset} for {Event} {Cameras}",
        "type": "article",
        "key": "bissarinova_faces_2024"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JPROC.2014.2346153",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC.2017.7870263",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC19947.2020.9063149",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2019.2941978",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2018.2793357",
            "source": "crossref"
        },
        {
            "doi": "10.1002/rob.21764",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ITSC.2006.1706816",
            "source": "crossref"
        },
        {
            "doi": "10.3389/frai.2022.1070964",
            "source": "crossref"
        },
        {
            "doi": "10.1177/0301006619838734",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2003.10057",
            "source": "crossref"
        },
        {
            "doi": "10.1523/JNEUROSCI.17-11-04302.1997",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-99010-1_25",
            "source": "crossref"
        },
        {
            "doi": "10.1017/S0263574721001739",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s12652-021-03352-0",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.patrec.2021.11.002",
            "source": "crossref"
        },
        {
            "doi": "10.1111/acer.14875",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.324",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-46478-7_31",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-018-1097-z",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.dsp.2020.102809",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIM.2018.2884364",
            "source": "crossref"
        },
        {
            "doi": "10.1109/SII52469.2022.9708901",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11042-020-08873-y",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICPR.2018.8545652",
            "source": "crossref"
        },
        {
            "doi": "10.3390/s21134520",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2021.3096136",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neucom.2018.03.030",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TBIOM.2021.3104014",
            "source": "crossref"
        },
        {
            "doi": "10.3233/FAIA230558",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2020.2998330",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.596",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.imavis.2013.12.004",
            "source": "crossref"
        },
        {
            "doi": "10.1109/BTAS.2018.8698561",
            "source": "crossref"
        },
        {
            "doi": "10.1109/FG.2018.00020",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR42600.2020.00525",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.527",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIFS.2022.3177949",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.inffus.2017.09.002",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.583",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3394171.3413726",
            "source": "crossref"
        },
        {
            "doi": "10.3390/s20247079",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00587",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TII.2022.3195063",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW59228.2023.00432",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2019.2963386",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.102",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-018-1106-2",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2021.03.019",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV.2016.7477561",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACVW50321.2020.9096944",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.116",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.690",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2015.169",
            "source": "crossref"
        },
        {
            "title": "A 128 \u00d7 128 120 dB 15 \u03bcs latency asynchronous temporal contrast vision sensor",
            "source": "crossref"
        },
        {
            "title": "Retinomorphic event-based vision sensors: Bioinspired cameras with spiking output",
            "source": "crossref"
        },
        {
            "title": "Event-based vision: A survey",
            "source": "crossref"
        },
        {
            "title": "IoT-guard: Event-driven fog-based video surveillance system for real-time security management",
            "source": "crossref"
        },
        {
            "title": "Ultimate SLAM? Combining events, images, and IMU for robust visual SLAM in HDR and high-speed scenarios",
            "source": "crossref"
        },
        {
            "title": "Vertical landing for micro air vehicles using event-based optical flow",
            "source": "crossref"
        },
        {
            "title": "Isolated single sound lip-reading using a frame-based camera and event-based camera",
            "source": "crossref"
        },
        {
            "title": "What is a face? Critical features for face detection",
            "source": "crossref"
        },
        {
            "title": "The fusiform face area: A module in human extrastriate cortex specialized for face perception",
            "source": "crossref"
        },
        {
            "title": "Application of 3D face recognition in the access control system",
            "source": "crossref"
        },
        {
            "title": "An interactive robot design to find missing people and inform their location by real-time face recognition system on moving images",
            "source": "crossref"
        },
        {
            "title": "An investigation on face detection applications",
            "source": "crossref"
        },
        {
            "title": "Head pose estimation using facial-landmarks classification for children rehabilitation games",
            "source": "crossref"
        },
        {
            "title": "Facial imaging to screen for fetal alcohol spectrum disorder: A scoping review",
            "source": "crossref"
        },
        {
            "title": "Learning to detect objects with a 1 megapixel event camera",
            "source": "crossref"
        },
        {
            "title": "Facial landmark detection: A literature survey",
            "source": "crossref"
        },
        {
            "title": "Face recognition: Past, present and future (a review)",
            "source": "crossref"
        },
        {
            "title": "A thermal infrared face database with facial landmarks and emotion labels",
            "source": "crossref"
        },
        {
            "title": "Joint learning for face alignment and face transfer with depth image",
            "source": "crossref"
        },
        {
            "title": "Recent advances in deep learning techniques for face recognition",
            "source": "crossref"
        },
        {
            "title": "Face detection using deep learning: An improved faster RCNN approach",
            "source": "crossref"
        },
        {
            "title": "ARFace: Attention-aware and regularization for face recognition with reinforcement learning",
            "source": "crossref"
        },
        {
            "title": "An exploratory analysis on visual counterfeits using Conv-LSTM hybrid architecture",
            "source": "crossref"
        },
        {
            "title": "Face detection by structural models",
            "source": "crossref"
        },
        {
            "title": "TFW: Annotated Thermal Faces in the Wild Dataset",
            "source": "crossref"
        },
        {
            "title": "Unconstrained Kinect video face database",
            "source": "crossref"
        },
        {
            "title": "Event-based face detection and tracking using the dynamics of Eye Blinks",
            "source": "crossref"
        },
        {
            "title": "Understanding human reactions looking at facial microexpressions with an event camera",
            "source": "crossref"
        },
        {
            "title": "High speed and high dynamic range video with an event camera",
            "source": "crossref"
        },
        {
            "title": "Simultaneous mosaicing and tracking with an event camera",
            "source": "crossref"
        },
        {
            "title": "Real-time intensity-image reconstruction for event cameras using manifold regularisation",
            "source": "crossref"
        },
        {
            "title": "Real-time face & eye tracking and blink detection using event cameras",
            "source": "crossref"
        },
        {
            "title": "Deep Residual Learning for Image Recognition",
            "source": "crossref"
        },
        {
            "title": "Convolutional LSTM Network: A machine learning approach for precipitation nowcasting",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

- 1.6 million faces
- [73] participants (31 female, 42 male)
- 629.2 Gb
- 689 min
- Data stored in Prophesee .raw files

The dataset contains a total of 689 min of event-based data, containing 1.6 million annotated faces from 73 participants, comprising 31 female and 42 male subjects. Data was collected in two scenarios: wild and controlled.

Total size is 629.2 Gb of raw event data, and 76.6 Gb of processed data.
