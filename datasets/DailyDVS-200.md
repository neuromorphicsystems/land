---
{
    "name": "DailyDVS-200",
    "aliases": [],
    "year": 2024,
    "modalities": [
        "Vision"
    ],
    "sensors": [
        "DVXplorer"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Human Action Recognition"
    ],
    "description": "Human Activity Recognition",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [
            "Baidu",
            "Google Drive"
        ],
        "file_formats": [
            "aedat"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Baidu",
                "url": "https://drive.google.com/drive/folders/1wJw_unn-oodoQzb-j6-q-IZWU3vehXsV",
                "format": "aedat",
                "available": true
            },
            {
                "name": "Google Drive",
                "url": "https://drive.google.com/drive/folders/1wJw_unn-oodoQzb-j6-q-IZWU3vehXsV",
                "format": "aedat",
                "available": true
            }
        ],
        "size_gb": 82.71,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "DailyDVS-200: A Comprehensive Benchmark Dataset for\u00a0Event-Based Action Recognition",
        "doi": "10.1007/978-3-031-72907-2_4",
        "authors": [
            "Qi Wang",
            "Zhou Xu",
            "Yuming Lin",
            "Jingtao Ye",
            "Hongsheng Li",
            "Guangming Zhu",
            "Syed Afaq Ali Shah",
            "Mohammed Bennamoun",
            "Liang Zhang"
        ],
        "abstract": "Neuromorphic sensors, specifically event cameras, revolutionize visual data acquisition by capturing pixel intensity changes with exceptional dynamic range, minimal latency, and energy efficiency, setting them apart from conventional frame-based cameras. The distinctive capabilities of event cameras have ignited significant interest in the domain of event-based action recognition, recognizing their vast potential for advancement. However, the development in this field is currently slowed by the lack of comprehensive, large-scale datasets, which are critical for developing robust recognition frameworks. To bridge this gap, we introduces DailyDVS-200, a meticulously curated benchmark dataset tailored for the event-based action recognition community. DailyDVS-200 is extensive, covering 200 action categories across real-world scenarios, recorded by 47 participants, and comprises more than 22,000 event sequences. This dataset is designed to reflect a broad spectrum of action types, scene complexities, and data acquisition diversity. Each sequence in the dataset is annotated with 14 attributes, ensuring a detailed characterization of the recorded actions. Moreover, DailyDVS-200 is structured to facilitate a wide range of research paths, offering a solid foundation for both validating existing approaches and inspiring novel methodologies. By setting a new benchmark in the field, we challenge the current limitations of neuromorphic data processing and invite a surge of new approaches in event-based action recognition techniques, which paves the way for future explorations in neuromorphic computing and beyond. The dataset and source code are available at https://github.com/QiWang233/DailyDVS-200.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 1,
            "updated": "2025-07-07T07:56:17.097212"
        },
        {
            "source": "scholar",
            "count": 5,
            "updated": "2025-07-07T07:56:16.845172"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2407.05106"
        },
        {
            "type": "paper",
            "url": "https://link.springer.com/chapter/10.1007/978-3-031-72907-2_4"
        },
        {
            "type": "github_page",
            "url": "https://github.com/QiWang233/DailyDVS-200"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_classes": "200",
        "num_subjects": "47",
        "num_recordings": "22046",
        "sensor_resolution": "320x240",
        "file_format": "aedat4"
    },
    "bibtex": {
        "pages": "55--72",
        "year": 2025,
        "editor": "Leonardis, Ale\u0161 and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G\u00fcl",
        "author": "Wang, Qi and Xu, Zhou and Lin, Yuming and Ye, Jingtao and Li, Hongsheng and Zhu, Guangming and Ali Shah, Syed Afaq and Bennamoun, Mohammed and Zhang, Liang",
        "publisher": "Springer Nature Switzerland",
        "booktitle": "Computer {Vision} \u2013 {ECCV} 2024",
        "doi": "10.1007/978-3-031-72907-2_4",
        "isbn": "978-3-031-72907-2",
        "title": "{DailyDVS}-200: {A} {Comprehensive} {Benchmark} {Dataset} for {Event}-{Based} {Action} {Recognition}",
        "address": "Cham",
        "type": "inproceedings",
        "key": "wang_dailydvs-200_2025"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/CVPR.2017.781",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2022.3172212",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIP.2020.3023597",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS55552.2023.10341740",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2015.7298698",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58565-5_9",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.502",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2019.00214",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.dib.2024.110340",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR42600.2020.00028",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00630",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2023.3300741",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00573",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00215",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2011.209",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-022-01594-9",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2011.6126543",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-005-1838-7",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2017.00309",
            "source": "crossref"
        },
        {
            "doi": "10.1609/aaai.v36i2.20021",
            "source": "crossref"
        },
        {
            "doi": "10.1038/s41597-022-01851-z",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00097",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00718",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2021.726582",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2011.5995353",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2019.2916873",
            "source": "crossref"
        },
        {
            "doi": "10.24963/ijcai.2021/240",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52688.2022.00320",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58598-3_25",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnbot.2019.00038",
            "source": "crossref"
        },
        {
            "doi": "10.1109/EBCCSP.2016.7605233",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2007.383299",
            "source": "crossref"
        },
        {
            "doi": "10.1109/MSP.2019.2931595",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00437",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV51070.2023.00555",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.5244/C.31.16",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW56347.2022.00301",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52688.2022.01205",
            "source": "crossref"
        },
        {
            "doi": "10.1145/1291233.1291311",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00481",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-010-0384-0",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00186",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2015.510",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00675",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-015-0846-5",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.patter.2023.100789",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00108",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00240",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52688.2022.00358",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

The dataset contains:

- 200 event-specific action categories
- 47 subjects
- 22046 video recordings
- Recorded with a DVXplorer sensor with a resolution of 320x240
- `14` attributes are labeled

The data is provided in aedat4 files.

## DailyDVS-200 Dataset Format:

In the DailyDVS-200 Dataset, we provide `all_data.json` file, which record the attributes of each data. An example are as follows:

```
{
    "FileName": "C0P3M0S1_20231111_09_11_23.aedat4",
    "Time": "20231111_09_11_23",
    "FilePath": ".../event_raw/11_11/3/C0P3M0S1_20231111_09_11_23.aedat4",
    "Scene": "1",
    "Action": "0",
    "Move": "0",
    "PersonNum": "1",
    "Range of Motion": "Limbs",
    "Complexity of Movement": "Easy",
    "Props/No Props": "No",
    "Indoor/Outdoor": "Indoor",
    "Background Complexity": "Easy",
    "Daytime/Nighttime": "Daytime",
    "Direction of Light": "Front Lighting",
    "Shadow": "No",
    "Standing/Sitting": "Standing",
    "Height": "Low",
    "Distance": "Near",
    "Perspective": "",
    "ID": "3"
}
```

In the DailyDVS-200 Dataset, which is provided in the .aedat4 format, the data is structured with 4 elements as follows:

- `t`: Represents the timestamp of the event.
- `x`: Represents the x-coordinate of the event.
- `y`: Represents the y-coordinate of the event.
- `p`: The polarity value. It contains three categories: 1 and 0. In our experiments, we consider 1 as positive polarity and 0 as negative polarity.

Source: https://github.com/QiWang233/DailyDVS-200#dailydvs-200-dataset-format
