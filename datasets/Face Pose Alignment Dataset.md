---
{
    "name": "Face Pose Alignment Dataset",
    "aliases": [],
    "year": 2020,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "ATIS"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Face Detection",
        "Facial Expression",
        "Emotion Recognition"
    ],
    "description": "Face Pose Alignment (Facial Detection)",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [],
        "availability_comment": "Dataset available on request",
        "dataset_links": []
    },
    "paper": {
        "title": "Face Pose Alignment with Event Cameras",
        "doi": "10.3390/s20247079",
        "authors": [
            "Arman Savran",
            "Chiara Bartolozzi"
        ],
        "abstract": "Event camera (EC) emerges as a bio-inspired sensor which can be an alternative or complementary vision modality with the benefits of energy efficiency, high dynamic range, and high temporal resolution coupled with activity dependent sparse sensing. In this study we investigate with ECs the problem of face pose alignment, which is an essential pre-processing stage for facial processing pipelines. EC-based alignment can unlock all these benefits in facial applications, especially where motion and dynamics carry the most relevant information due to the temporal change event sensing. We specifically aim at efficient processing by developing a coarse alignment method to handle large pose variations in facial applications. For this purpose, we have prepared by multiple human annotations a dataset of extreme head rotations with varying motion intensity. We propose a motion detection based alignment approach in order to generate activity dependent pose-events that prevents unnecessary computations in the absence of pose change. The alignment is realized by cascaded regression of extremely randomized trees. Since EC sensors perform temporal differentiation, we characterize the performance of the alignment in terms of different levels of head movement speeds and face localization uncertainty ranges as well as face resolution and predictor complexity. Our method obtained 2.7\\% alignment failure on average, whereas annotator disagreement was 1\\%. The promising coarse alignment performance on EC sensor data together with a comprehensive analysis demonstrate the potential of ECs in facial applications.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 13,
            "updated": "2025-06-29T07:42:32.218744"
        },
        {
            "source": "scholar",
            "count": 18,
            "updated": "2025-06-29T07:42:31.894807"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://www.mdpi.com/1424-8220/20/24/7079"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_recordings": "108",
        "tota_duration": "10.2 min",
        "num_subjects": "18",
        "num_male": "9",
        "num_female": "9",
        "stereo": false
    },
    "bibtex": {
        "pages": "7079",
        "year": 2020,
        "month": "dec",
        "author": "Savran, Arman and Bartolozzi, Chiara",
        "journal": "Sensors",
        "urldate": "2024-04-13",
        "number": "24",
        "language": "en",
        "doi": "10.3390/s20247079",
        "url": "https://www.mdpi.com/1424-8220/20/24/7079",
        "issn": "1424-8220",
        "copyright": "https://creativecommons.org/licenses/by/4.0/",
        "volume": "20",
        "title": "Face {Pose} {Alignment} with {Event} {Cameras}",
        "type": "article",
        "key": "savran_face_2020"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-018-1097-z",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2017.08.008",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.116",
            "source": "crossref"
        },
        {
            "doi": "10.1109/FG.2018.00055",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2019.8702565",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2014.241",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2014.218",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2017.2778152",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00529",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2017.2787130",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00019",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00045",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00088",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00167",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00238",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-018-1095-1",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.181",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.347",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.429",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2017.2734779",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.393",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.400",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.167",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.453",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-10605-2_1",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.373",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2015.417",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.01049",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2020.3015759",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00398",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00568",
            "source": "crossref"
        },
        {
            "doi": "10.1214/aos/1013203451",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2010.5540094",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2013.191",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2013.75",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-013-0667-3",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-10599-4_8",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCVW.2015.130",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2015.7299048",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-017-0999-5",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCVW.2015.131",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-46484-8_39",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.409",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00573",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV.2016.7477561",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2015.2401834",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00587",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.102",
            "source": "crossref"
        },
        {
            "doi": "10.1177/0278364917691115",
            "source": "crossref"
        },
        {
            "doi": "10.1007/BF02291478",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s10994-006-6226-1",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.imavis.2016.01.002",
            "source": "crossref"
        },
        {
            "doi": "10.1162/NECO_a_00703",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV45572.2020.9093366",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC.2017.7870263",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1017/CBO9781107326279",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2018.2793357",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2017.8050397",
            "source": "crossref"
        },
        {
            "title": "A 240 \u00d7 180 130 dB 3 \u03bcs Latency Global Shutter Spatiotemporal Vision Sensor",
            "source": "crossref"
        },
        {
            "title": "A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS",
            "source": "crossref"
        },
        {
            "title": "Facial Landmark Detection: A Literature Survey",
            "source": "crossref"
        },
        {
            "title": "Face Alignment In-the-wild",
            "source": "crossref"
        },
        {
            "title": "HOTS: A Hierarchy Of event-based Time-Surfaces for pattern recognition",
            "source": "crossref"
        },
        {
            "title": "Face Alignment in Full Pose Range: A 3D Total Solution",
            "source": "crossref"
        },
        {
            "title": "Facial Landmark Detection with Tweaked Convolutional Neural Networks",
            "source": "crossref"
        },
        {
            "title": "RED-Net: A Recurrent Encoder-Decoder Network for Video-Based Face Alignment",
            "source": "crossref"
        },
        {
            "title": "Two-Stream Transformer Networks for Video-Based Face Alignment",
            "source": "crossref"
        },
        {
            "title": "Hybrid Deblur Net: Deep Non-Uniform Deblurring with Event Camera",
            "source": "crossref"
        },
        {
            "title": "Greedy Function Approximation: A Gradient Boosting Machine",
            "source": "crossref"
        },
        {
            "title": "Face Alignment by Explicit Shape Regression",
            "source": "crossref"
        },
        {
            "title": "A Comprehensive Performance Evaluation of Deformable Face Tracking \u201cIn-the-Wild\u201d",
            "source": "crossref"
        },
        {
            "title": "An Asynchronous Neuromorphic Event-Driven Visual Part-Based Shape Tracking",
            "source": "crossref"
        },
        {
            "title": "Event-Based Face Detection and Tracking Using the Dynamics of Eye Blinks",
            "source": "crossref"
        },
        {
            "title": "The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM",
            "source": "crossref"
        },
        {
            "title": "Generalized procrustes analysis",
            "source": "crossref"
        },
        {
            "title": "Extremely Randomized Trees",
            "source": "crossref"
        },
        {
            "title": "300 Faces In-The-Wild Challenge",
            "source": "crossref"
        },
        {
            "title": "What can neuromorphic event-driven precise timing add to spike-based pattern recognition?",
            "source": "crossref"
        },
        {
            "title": "A 128 \u00d7 128 120 dB 15 \u03bcs Latency Asynchronous Temporal Contrast Vision Sensor",
            "source": "crossref"
        },
        {
            "title": "Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High-Speed Scenarios",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

In the dataset there are 108 clips of 10.2 min in total, from 18 subjects (9 males and 9 females).
