---
{
    "name": "RG-Event",
    "aliases": [],
    "year": 2024,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DVXplorer"
    ],
    "other_sensors": [],
    "category": "Robotic and Moving Vehicle Datasets",
    "tags": [
        "Transformer",
        "Robotics"
    ],
    "description": "Robotic Gripper Force Measurement",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [],
        "availability_comment": "",
        "dataset_links": []
    },
    "paper": {
        "title": "Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-Based Vision Transformer",
        "doi": "10.1109/ReMAR61031.2024.10617687",
        "authors": [
            "Qianyu Guo",
            "Ziqing Yu",
            "Jiaming Fu",
            "Yawen Lu",
            "Yahya Zweiri",
            "Dongming Gan"
        ],
        "abstract": "Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 1,
            "updated": "2025-07-03T09:57:00.242070"
        },
        {
            "source": "scholar",
            "count": 2,
            "updated": "2025-07-03T09:56:59.719056"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2404.01170"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/10617687"
        }
    ],
    "full_name": "",
    "bibtex": {
        "pages": "608--613",
        "year": 2024,
        "month": "jun",
        "author": "Guo, Qianyu and Yu, Ziqing and Fu, Jiaming and Lu, Yawen and Zweiri, Yahya and Gan, Dongming",
        "publisher": "IEEE",
        "booktitle": "2024 6th {International} {Conference} on {Reconfigurable} {Mechanisms} and {Robots} ({ReMAR})",
        "urldate": "2024-12-14",
        "language": "en",
        "doi": "10.1109/ReMAR61031.2024.10617687",
        "url": "https://ieeexplore.ieee.org/document/10617687/",
        "shorttitle": "Force-{EvT}",
        "isbn": "9798350395969",
        "copyright": "https://doi.org/10.15223/policy-029",
        "title": "Force-{EvT}: {A} {Closer} {Look} at {Robotic} {Gripper} {Force} {Measurement} with {Event}-{Based} {Vision} {Transformer}",
        "address": "Chicago, IL, USA",
        "type": "inproceedings",
        "key": "guo_force-evt_2024"
    },
    "referenced_papers": [
        {
            "doi": "10.3390/s20164469",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.102",
            "source": "crossref"
        },
        {
            "doi": "10.1016/S0304-4076(96)01818-0",
            "source": "crossref"
        },
        {
            "doi": "10.5194/gmd-7-1247-2014",
            "source": "crossref"
        },
        {
            "doi": "10.1243/0954406001523696",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICAICA58456.2023.10405429",
            "source": "crossref"
        },
        {
            "doi": "10.1115/DETC2023-117322",
            "source": "crossref"
        },
        {
            "doi": "10.1017/S026357472300156X",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-68780-9_51",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMECH.2002.802720",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSEN.2017.2788015",
            "source": "crossref"
        },
        {
            "doi": "10.3390/technologies9010008",
            "source": "crossref"
        },
        {
            "doi": "10.1533/9780857095763.1.143",
            "source": "crossref"
        },
        {
            "doi": "10.1109/GlobalSIP45357.2019.8969077",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2021.3060699",
            "source": "crossref"
        },
        {
            "doi": "10.1109/VRW62533.2024.00025",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52729.2023.01732",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00568",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCVW54120.2021.00103",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2020.3017738",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIM.2019.2919354",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TITS.2022.3233801",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.01196",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-017-1050-6",
            "source": "crossref"
        },
        {
            "doi": "10.4236/jcc.2019.73002",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV45572.2020.9093366",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00359",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00061",
            "source": "crossref"
        },
        {
            "doi": "10.55524/ijircst.2024.12.1.6",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-89370-5_26",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00256",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TRO.2021.3060971",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CYBER.2018.8688163",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-27526-6_30",
            "source": "crossref"
        },
        {
            "title": "A review: State of the art of robotic grippers",
            "source": "crossref"
        },
        {
            "title": "Learning to detect objects with a 1 megapixel event camera",
            "source": "crossref"
        },
        {
            "title": "Long-term object tracking with a moving event camera",
            "source": "crossref"
        }
    ]
}
---

=== Dataset Structure
