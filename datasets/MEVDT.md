---
{
    "name": "MEVDT",
    "aliases": [],
    "year": 2024,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DAVIS240"
    ],
    "other_sensors": [],
    "category": "Object Detection, Classification, and Tracking",
    "tags": [
        "Traffic Monitoring",
        "Vehicle Detection",
        "Object Tracking"
    ],
    "description": "Object Detection and Tracking on traffic dataset",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [
            "Direct Download"
        ],
        "file_formats": [
            "CSV"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Direct Download",
                "url": "https://deepblue.lib.umich.edu/data/concern/data_sets/bc386k045",
                "format": "CSV",
                "available": true,
                "doi": "https://doi.org/10.7302/d5k3-9150"
            }
        ],
        "size_gb": 1.26,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "MEVDT: Multi-modal event-based vehicle detection and tracking dataset",
        "doi": "10.1016/j.dib.2024.111205",
        "authors": [
            "Zaid A. El Shair",
            "Samir A. Rawashdeh"
        ],
        "abstract": "In this data article, we introduce the Multi-Modal Event-based Vehicle Detection and Tracking (MEVDT) dataset. This dataset provides a synchronized stream of event data and grayscale images of traffic scenes, captured using the Dynamic and Active-Pixel Vision Sensor (DAVIS) 240c hybrid event-based camera. MEVDT comprises 63 multi-modal sequences with approximately 13k images, 5M events, 10k object labels, and 85 unique object tracking trajectories. Additionally, MEVDT includes manually annotated ground truth labels \u2014 consisting of object classifications, pixel-precise bounding boxes, and unique object IDs \u2014 which are provided at a labeling frequency of 24 Hz. Designed to advance the research in the domain of event-based vision, MEVDT aims to address the critical need for high-quality, real-world annotated datasets that enable the development and evaluation of object detection and tracking algorithms in automotive environments.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 0,
            "updated": "2025-06-30T07:06:05.671616"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2407.20446"
        },
        {
            "type": "paper",
            "url": "https://www.sciencedirect.com/science/article/pii/S2352340924011673"
        },
        {
            "type": "project_page",
            "url": "http://sar-lab.net/event-based-vehicle-detection-and-tracking-dataset/"
        }
    ],
    "full_name": "Multi-modal event-based vehicle detection and tracking dataset",
    "additional_metadata": {
        "stereo": false
    },
    "bibtex": {
        "pages": "210",
        "year": 2022,
        "month": "jul",
        "author": "El Shair, Zaid and Rawashdeh, Samir A.",
        "journal": "Journal of Imaging",
        "urldate": "2024-04-13",
        "number": "8",
        "language": "en",
        "doi": "10.3390/jimaging8080210",
        "url": "https://www.mdpi.com/2313-433X/8/8/210",
        "issn": "2313-433X",
        "copyright": "https://creativecommons.org/licenses/by/4.0/",
        "volume": "8",
        "title": "High-{Temporal}-{Resolution} {Object} {Detection} and {Tracking} {Using} {Images} and {Events}",
        "type": "article",
        "key": "el_shair_high-temporal-resolution_2022"
    },
    "referenced_papers": [
        {
            "doi": "10.3390/jimaging8080210",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2023.3298925",
            "source": "crossref"
        },
        {
            "doi": "10.1177/0278364917691115",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3593587",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/MITS.2019.2907630",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnbot.2019.00038",
            "source": "crossref"
        },
        {
            "doi": "10.1117/1.OE.62.3.031209",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-020-01393-0",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "title": "High-temporal-resolution object detection and tracking using images and events",
            "source": "crossref"
        },
        {
            "title": "Event-based vision: a survey",
            "source": "crossref"
        },
        {
            "title": "Learning to detect objects with a 1 megapixel event camera",
            "source": "crossref"
        },
        {
            "title": "Sodformer: streaming object detection with transformer using events and frames",
            "source": "crossref"
        },
        {
            "title": "The event-camera dataset and simulator: event-based data for pose estimation, visual odometry, and slam",
            "source": "crossref"
        },
        {
            "title": "A reconfigurable architecture for real-time event-based multi-object tracking",
            "source": "crossref"
        },
        {
            "title": "Hats: histograms of averaged time surfaces for robust event-based object classification",
            "source": "crossref"
        },
        {
            "title": "A 128\u00d7128 120 db 15 \u00b5s latency asynchronous temporal contrast vision sensor",
            "source": "crossref"
        },
        {
            "title": "A review of sensor technologies for perception in automated driving",
            "source": "crossref"
        },
        {
            "title": "Neuromorphic vision datasets for pedestrian detection, action recognition, and fall detection",
            "source": "crossref"
        },
        {
            "title": "High-temporal-resolution event-based vehicle detection and tracking",
            "source": "crossref"
        },
        {
            "title": "A low power, fully event-based gesture recognition system",
            "source": "crossref"
        },
        {
            "title": "Microsoft coco: common objects in context",
            "source": "crossref"
        },
        {
            "title": "Motchallenge: a benchmark for single-camera multiple target tracking",
            "source": "crossref"
        },
        {
            "title": "Event-based, 6-dof pose tracking for high-speed maneuvers",
            "source": "crossref"
        },
        {
            "title": "A 240\u00d7 180 130 db 3 \u00b5s latency global shutter spatiotemporal vision sensor",
            "source": "crossref"
        }
    ]
}
---

### Dataset Description

Key Points:

- The MEVDT dataset addresses the critical need for annotated datasets in the domain of event-based computer vision, particularly for automotive applications.
- The dataset comprises 63 multi-modal traffic sequences featuring approximately 13k images, 5M of high-temporal resolution events, 10k manually-annotated object labels, and 85 unique object tracking trajectories.
- Labels for object detection and tracking, alongside multiple data formats, are included to support the development of advanced computer vision algorithms.

Research Overview:
The MEVDT dataset was developed to support advancements in event-based computer vision by providing a rich source of annotated visual data specifically targeting the automotive domain. Its primary goal is to enable the development of event-based and multi-modal algorithms focused on object detection and tracking in dynamic traffic environments, leveraging the unique capabilities of event-based sensors. This dataset includes a significant volume of labeled data essential for advancing research in a novel field where such resources are scarce.

Methodology:
Data was collected using the hybrid sensor DAVIS 240c, which combines an Active Pixel Sensor (APS) and a Dynamic Vision Sensor (DVS) within the same pixel array. The APS captures grayscale images at 24 FPS, while the DVS records pixel brightness changes (i.e., events) at microsecond resolution. The collection process took place at the University of Michigan-Dearborn campus, in two scenes under clear daylight conditions. Data recording was managed using the Robot Operating System (ROS) DVS package running on a laptop. The camera was fixed capturing moving vehicles which were manually labeled with 2D bounding boxes and unique IDs.

Files contained here:
The MEVDT dataset is organized into four directories, each with training and testing splits:

- sequences/: Holds grayscale images and sequence-long event streams. Within each scene, sequences are titled by the first sample's timestamp, including image files (.png) and events stream lists (.csv).
- labels/: Contains ground truth data for object detection and tracking computer vision tasks. The tracking_labels/ includes annotations in formats like COCO JSON, MOT Challenge, and a custom format for sequence-long object tracking; while detection_labels/ provides per-image object class and bounding box coordinates in x_min, y_min, x_max, y_max format for each event sample file in event_samples/.
- event_samples/: Consists of fixed-duration batch-sampled event files in AEDAT format, organized by sampling interval, enabling research at various temporal resolutions.
- data_splits/: Features CSV files that index the dataset splits for object detection, guiding users through the existing training and testing structure for different sampling durations.

Note: each line in the sequence-long event stream CSV files corresponds to a single event in comma-separated in the following format: ts, x, y, p.

In this format, ts denotes the event's timestamp in nanoseconds, x and y correspond to its two-dimensional pixel coordinate, and p indicates its polarity as either positive (p=1) or negative (p=0).

### Dataset Structure

```


MEVDT/							Root directory: contains all data and labels for MEVDT dataset
├── sequences/						Contains images and sequence-long event streams, organized into training and testing datasets
│ ├── train/
│ │ ├── Scene_A/
│ │ │ └── <sequence_name>/				Directory named after the timestamp of its first sample; holds related images and event data
│ │ │ ├── <image_timestamp>.png 			Image file named by its exact timestamp of capture
│ │ │ └── <sequence_name>_events.csv 			Event stream CSV file listing events captured throughout the sequence
│ │ └── Scene_B/ 					Structure follows the same format as Scene_A/
│ │ │ └── ...
│ └── test/ 						Testing data, organized similarly to the training data in train/
| └── ...
|
├── labels/ 						Ground truth data for object tracking and detection tasks
│ ├── tracking_labels/ 					Object tracking labels
│ │ ├── train/
│ │ │ └── Scene_A/
│ │ │ └── <sequence_name>/ 				Directory named after the timestamp of its first sample, containing ground truth in various formats
│ │ │ ├── <sequence_name>-coco.json 			Ground truth in COCO JSON format
│ │ │ ├── <sequence_name>-custom<tracking_rate>.txt 	Custom ground truth format for tracking at specified rates {24, 48, 96, 192, 384 Hz}
│ │ │ └── <sequence_name>-mot<tracking_rate>.txt 	MOT challenge format ground truth for specified tracking rates {24, 48, 96, 192, 384 Hz}
│ │ └── ... 						Repeat structure for additional scenes and sequences as in sequences/
│ │
│ └── detection_labels/ 				Object detection labels
│ ├── train/
│ │ └── Scene_A/
│ │ └── <sequence_name>/ 				Directory for each sequence, containing a detection labels file per image/sample
│ │ └── <sample_timestamp>.txt 				Detection ground truth; format specifies object class and bounding box coordinates per image
│ └── ... 						Repeat structure for additional scenes and sequences as in sequences/
|
├── event_samples/ 					Sampled event data files at different batch-sampling durations
│ └── <sample_duration>/
│ ├── train/
│ │ └── Scene_A/
│ │ └── <sequence_name>/ 				Contains event data files sampled according to the specified duration
│ │ └── <sample_timestamp>.aedat 			Sampled event data file encoded in .aedat format
│ └── ... 						Repeat structure for additional scenes and sequences as in sequences/
|
└── data_splits/ 					CSV files for indexing dataset splits for object detection
  ├── MEVDT_<sample_duration>_train.csv 		Training set index file for each sampling duration {original rate, 100, 200, 500 ms}
  └── MEVDT_<sample_duration>_test.csv 			Testing set index file for each sampling duration {original rate, 100, 200, 500 ms}
```
