---
{
    "name": "Wang2020",
    "aliases": [],
    "year": 2020,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "ESIM"
    ],
    "other_sensors": [],
    "category": "Intensity Reconstruction, Optical Flow, and Frame Fusion",
    "tags": [
        "HDR Video Reconstruction",
        "High-Speed Video Reconstruction",
        "Frame Reconstruction"
    ],
    "description": "Frame/Event fusion (HDR and high temporal imaging)",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": false,
        "has_simulated_data": true,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [
            "Baidu"
        ],
        "file_formats": [
            "Matlab"
        ],
        "availability_comment": "",
        "dataset_links": [],
        "size_gb": 17.2,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "Event Enhanced High-Quality Image Recovery",
        "doi": "10.1007/978-3-030-58601-0_10",
        "authors": [
            "Bishan Wang",
            "Jingwei He",
            "Lei Yu",
            "Gui-Song Xia",
            "Wen Yang"
        ],
        "abstract": "With extremely high temporal resolution, event cameras have a large potential for robotics and computer vision. However, their asynchronous imaging mechanism often aggravates the measurement sensitivity to noises and brings a physical burden to increase the image spatial resolution. To recover high-quality intensity images, one should address both denoising and super-resolution problems for event cameras. Since events depict brightness changes, with the enhanced degeneration model by the events, the clear and sharp high-resolution latent images can be recovered from the noisy, blurry and low-resolution intensity observations. Exploiting the framework of sparse learning, the events and the low-resolution intensity observations can be jointly considered. Based on this, we propose an explainable network, an event-enhanced sparse learning network (eSL-Net), to recover the high-quality images from event cameras. After training with a synthetic dataset, the proposed eSL-Net can largely improve the performance of the state-of-the-art by 7-12 dB. Furthermore, without additional training process, the proposed eSL-Net can be easily extended to generate continuous frames with frame-rate as high as the events.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 72,
            "updated": "2025-06-05T22:40:43.933198"
        },
        {
            "source": "scholar",
            "count": 142,
            "updated": "2025-06-05T22:40:42.994583"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2007.08336"
        },
        {
            "type": "paper",
            "url": "https://link.springer.com/chapter/10.1007/978-3-030-58601-0_10"
        },
        {
            "type": "github_page",
            "url": "https://github.com/ShinyWang33/eSL-Net"
        }
    ],
    "full_name": "Event-enhanced Sparse Learning Network",
    "additional_metadata": {
        "stereo": false
    },
    "bibtex": {
        "pages": "155--171",
        "note": "Series Title: Lecture Notes in Computer Science",
        "doi": "10.1007/978-3-030-58601-0_10",
        "year": 2020,
        "editor": "Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael",
        "author": "Wang, Bishan and He, Jingwei and Yu, Lei and Xia, Gui-Song and Yang, Wen",
        "publisher": "Springer International Publishing",
        "booktitle": "Computer {Vision} \u2013 {ECCV} 2020",
        "urldate": "2024-12-15",
        "language": "en",
        "url": "https://link.springer.com/10.1007/978-3-030-58601-0_10",
        "isbn": "978-3-030-58600-3 978-3-030-58601-0",
        "volume": "12358",
        "title": "Event {Enhanced} {High}-{Quality} {Image} {Recovery}",
        "address": "Cham",
        "type": "incollection",
        "key": "Wang2020"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/TCI.2019.2948787",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.102",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV.2016.7477561",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IJCNN.2011.6033299",
            "source": "crossref"
        },
        {
            "doi": "10.1002/cpa.20042",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2015.2439281",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIT.2006.871582",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIP.2006.881969",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00573",
            "source": "crossref"
        },
        {
            "doi": "10.5244/C.28.26",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-46466-4_21",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.conb.2010.03.007",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-018-1106-2",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2019.00251",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.35",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.37",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.180",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3036667",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00698",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00398",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-20873-8_20",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC.2017.7870263",
            "source": "crossref"
        },
        {
            "doi": "10.1111/j.2517-6161.1996.tb02080.x",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2018.2793357",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCVW.2019.00532",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.34",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIP.2017.2662206",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-01234-2_18",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure
