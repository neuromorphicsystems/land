---
{
    "name": "EV-CASIA-B",
    "aliases": [],
    "year": 2020,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DVS128"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "Gait Recognition",
        "Monitor Conversion"
    ],
    "task": "Gait Recognition",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": true,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [],
        "availability_comment": "Dataset is marked as public but no dataset link was provided.",
        "dataset_links": []
    },
    "paper": {
        "title": "EV-Gait: Event-Based Robust Gait Recognition Using Dynamic Vision Sensors",
        "doi": "10.1109/CVPR.2019.00652",
        "authors": [
            "Yanxiang Wang",
            "Bowen Du",
            "Yiran Shen",
            "Kai Wu",
            "Guangrong Zhao",
            "Jianguo Sun",
            "Hongkai Wen"
        ],
        "abstract": "In this paper, we introduce a new type of sensing modality, the Dynamic Vision Sensors (Event Cameras), for the task of gait recognition. Compared with the traditional RGB sensors, the event cameras have many unique advantages such as ultra low resources consumption, high temporal resolution and much larger dynamic range. However, those cameras only produce noisy and asynchronous events of intensity changes rather than frames, where conventional vision-based gait recognition algorithms can\u2019t be directly applied. To address this, we propose a new Event-based Gait Recognition (EV-Gait) approach, which exploits motion consistency to effectively remove noise, and uses a deep neural network to recognise gait from the event streams. To evaluate the performance of EV-Gait, we collect two event-based gait datasets, one from real-world experiments and the other by converting the publicly available RGB gait recognition benchmark CASIA-B. Extensive experiments show that EV-Gait can get nearly 96% recognition accuracy in the real-world settings, while on the CASIA-B benchmark it achieves comparable performance with state-of-the-art RGB-based gait recognition approaches.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 113,
            "updated": "2025-06-24T06:07:19.557815"
        },
        {
            "source": "scholar",
            "count": 191,
            "updated": "2025-06-24T06:07:18.671178"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_EV-Gait_Event-Based_Robust_Gait_Recognition_Using_Dynamic_Vision_Sensors_CVPR_2019_paper.pdf"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/abstract/document/8953966"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "source_dataset": "CASIA-B",
        "num_subjects": "124",
        "stereo": false
    },
    "bibtex": {
        "doi": "10.1109/CVPR.2019.00652",
        "keywords": "Deep learning;Tracking;Neural networks;Benchmark testing;Vision sensors;Sensor phenomena and characterization;Cameras;Vision Applications and Systems;Face;Gesture;and Body Pose;Others;Recognition: Detection;Categorization;Retrieval",
        "pages": "6351-6360",
        "number": "",
        "volume": "",
        "year": 2019,
        "title": "EV-Gait: Event-Based Robust Gait Recognition Using Dynamic Vision Sensors",
        "booktitle": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "author": "Wang, Yanxiang and Du, Bowen and Shen, Yiran and Wu, Kai and Zhao, Guangrong and Sun, Jianguo and Wen, Hongkai",
        "type": "inproceedings",
        "key": "Wang2020"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/CVPR.2018.00186",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICB.2016.7550060",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2018.00118",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ECMR.2015.7324048",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICPR.2004.1333741",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICIP.2015.7350936",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2015.7169170",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2007.1096",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICFSP.2016.7802963",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TSMCB.2009.2031091",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2006.38",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.90",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2016.00405",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TETC.2017.2788865",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2015.7168735",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.781",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2017.10.004",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2013.2273537",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.conb.2010.03.007",
            "source": "crossref"
        },
        {
            "doi": "10.1049/ic.2009.0230",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2009.5117867",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2013.00223",
            "source": "crossref"
        },
        {
            "doi": "10.15607/RSS.2018.XIV.062",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS.2016.7758089",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2003.1251144",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIP.2003.815251",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCSVT.2012.2186744",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2545669",
            "source": "crossref"
        },
        {
            "doi": "10.1109/AFGR.2002.1004148",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICIP.2016.7533144",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.patcog.2010.10.011",
            "source": "crossref"
        },
        {
            "title": "Rectified linear units improve restricted boltzmann machines",
            "source": "crossref"
        },
        {
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "source": "crossref"
        },
        {
            "title": "Real-time visualinertial odometry for event cameras using keyframe-based nonlinear optimization",
            "source": "crossref"
        },
        {
            "title": "Fast r-cnn",
            "source": "crossref"
        },
        {
            "title": "Real-time 3d reconstruction and 6-dof tracking with an event camera",
            "source": "crossref"
        },
        {
            "title": "A 240&#x00D7;180 10mW 12us latency sparse-output vision sensor for mobile applications",
            "source": "crossref"
        },
        {
            "title": "Tensorflow: a system for large-scale machine learning",
            "source": "crossref"
        },
        {
            "title": "Adam: A method for stochastic optimization",
            "source": "crossref"
        },
        {
            "title": "A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition",
            "source": "crossref"
        },
        {
            "title": "Imagenet classification with deep convolutional neural networks",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

- Converted from the CASIA-B dataset.
- Dataset contains 124 subjects, each of which has 66 video clips recorded by RGB camera from 11 different view angles (0◦ to 180◦), i.e., 6 clips for each angle. The view angle is the relative angle between the view of the camera and walking direction of the subjects.
