---
{
    "name": "N-HAR",
    "aliases": [],
    "year": 2019,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "ATIS"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "None"
    ],
    "task": "Human Activity Recognition",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": false,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [],
        "availability_comment": "Dataset link is not currently working",
        "dataset_links": []
    },
    "paper": {
        "title": "N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces",
        "doi": "10.1109/ISCAS.2019.8702581",
        "authors": [
            "Bibrat Ranjan Pradhan",
            "Yeshwanth Bethi",
            "Sathyaprakash Narayanan",
            "Anirban Chakraborty",
            "Chetan Singh Thakur"
        ],
        "abstract": "In recent years, a new generation of low-power, neuromorphic, event-based vision sensors has been gaining popularity for their very low latency and data sparsity. Though the conventional frame-based cameras have advanced in a lot of ways, they suffer from data redundancy and temporal latency. The bio-inspired arti\ufb01cial retinas eliminate the data redundancy by capturing only the change in illumination at each pixel and asynchronously communicating in binary spikes. In this work, we propose a system to achieve the task of human activity recognition based on the event-based camera data. We show that such tasks, which generally need high frame rate sensors for accurate predictions, can be achieved by adapting existing computer vision techniques to the spiking domain. We used event memory surfaces to make the sparse event data compatible with deep convolutional neural networks (CNNs). We leverage upon the recent advances in deep convolutional networks based video analysis and adapt such frameworks onto the neuromorphic domain. We also provide the community with a new dataset consisting of \ufb01ve categories of human activities captured in real world without any simulations. We achieved an accuracy of 94.3\\% using event memory surfaces on our activity recognition dataset.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 19,
            "updated": "2025-06-14T10:15:54.455643"
        },
        {
            "source": "scholar",
            "count": 31,
            "updated": "2025-06-14T10:15:54.288065"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://labs.dese.iisc.ac.in/neuronics/wp-content/uploads/sites/16/2019/03/n-HAR-CR-Final-2.pdf"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/8702581"
        },
        {
            "type": "project_page",
            "url": "http://neuronics.dese.iisc.ac.in/research/research-highlights/n-har/"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_classes": "5",
        "num_recordings": "3091",
        "stereo": false
    },
    "bibtex": {
        "pages": "1--5",
        "year": 2019,
        "month": "may",
        "author": "Pradhan, Bibrat Ranjan and Bethi, Yeshwanth and Narayanan, Sathyaprakash and Chakraborty, Anirban and Thakur, Chetan Singh",
        "publisher": "IEEE",
        "booktitle": "2019 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})",
        "urldate": "2024-03-04",
        "language": "en",
        "doi": "10.1109/ISCAS.2019.8702581",
        "url": "https://ieeexplore.ieee.org/document/8702581/",
        "shorttitle": "N-{HAR}",
        "isbn": "978-1-72810-397-6",
        "title": "N-{HAR}: {A} {Neuromorphic} {Event}-{Based} {Human} {Activity} {Recognition} {System} using {Memory} {Surfaces}",
        "address": "Sapporo, Japan",
        "type": "inproceedings",
        "key": "pradhan_n-har_2019"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2009.5206848",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2015.7298878",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.502",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.213",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-46484-8_2",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2015.510",
            "source": "crossref"
        },
        {
            "doi": "10.1038/srep14730",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00046",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2014.2352401",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.proeng.2012.04.128",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/82.842110",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2014.2308551",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2011.11.001",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LSP.2016.2611485",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2016.00405",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.308",
            "source": "crossref"
        },
        {
            "title": "Event-based sensing for space situational awareness",
            "source": "crossref"
        },
        {
            "title": "Investigation of event-based memory surfaces for high-speed tracking, unsupervised feature extraction and object recognition",
            "source": "crossref"
        },
        {
            "title": "Two-stream convolutional networks for action recognition in videos",
            "source": "crossref"
        },
        {
            "title": "Recognizing human actions: a local svm approach",
            "source": "crossref"
        },
        {
            "title": "Adam: A method for stochastic optimization",
            "source": "crossref"
        },
        {
            "title": "Tensorflow&#x2013;a system for large-scale machine learning",
            "source": "crossref"
        }
    ]
}
---


### Dataset Structure 

Thirty subjects with diversity in height and gender were part of the dataset collection. Each person performed 5 categories of activities and then event memory surfaces of each clip were generated and quantized at a rate of 30 FPS. 

The dataset contains 3091: 
- Boxing: 475, 
- Clapping: 435, 
- Jogging: 860, 
- Walking: 791, 
- Waving: 530.