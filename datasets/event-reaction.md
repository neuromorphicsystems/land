---
{
    "name": "event-reaction",
    "aliases": [],
    "year": 2022,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "Prophesee Gen3"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Emotion Recognition",
        "Facial Expression"
    ],
    "description": "Facial Expression Recognition",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [],
        "availability_comment": "",
        "dataset_links": []
    },
    "paper": {
        "title": "Understanding Human Reactions Looking at Facial Microexpressions With an Event Camera",
        "doi": "10.1109/TII.2022.3195063",
        "authors": [
            "Federico Becattini",
            "Federico Palai",
            "Alberto Del Bimbo"
        ],
        "abstract": "With the establishment of Industry 4.0, machines are now required to interact with workers. By observing biometrics they can assess if humans are authorized, or mentally and physically \ufb01t to work. Understanding body language, makes human\u2013machine interaction more natural, secure, and effective. Nonetheless, traditional cameras have limitations; low frame rate and dynamic range hinder a comprehensive human understanding. This poses a challenge, since faces undergo frequent instantaneous microexpressions. In addition, this is privacy-sensitive information that must be protected. We propose to model expressions with event cameras, bio-inspired vision sensors that have found application within the Industry 4.0 scope. They capture motion at millisecond rates and work under challenging conditions like low illumination and highly dynamic scenes. Such cameras are also privacy-preserving, making them extremely interesting for industry. We show that using event cameras, we can understand human reactions by only observing facial expressions. Comparison with red-green-blue (RGB)-based modeling demonstrates improved effectiveness and robustness.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 29,
            "updated": "2025-07-11T21:23:49.872592"
        },
        {
            "source": "scholar",
            "count": 39,
            "updated": "2025-07-11T21:23:49.552588"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/abstract/document/9844855"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_subjects": "25",
        "num_recordings": "455",
        "stereo": false
    },
    "bibtex": {
        "pages": "9112--9121",
        "year": 2022,
        "month": "dec",
        "author": "Becattini, Federico and Palai, Federico and Bimbo, Alberto Del",
        "journal": "IEEE Transactions on Industrial Informatics",
        "urldate": "2024-04-13",
        "number": "12",
        "language": "en",
        "doi": "10.1109/TII.2022.3195063",
        "url": "https://ieeexplore.ieee.org/document/9844855/",
        "issn": "1551-3203, 1941-0050",
        "copyright": "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
        "volume": "18",
        "title": "Understanding {Human} {Reactions} {Looking} at {Facial} {Microexpressions} {With} an {Event} {Camera}",
        "type": "article",
        "key": "becattini_understanding_2022"
    },
    "referenced_papers": [
        {
            "doi": "10.3390/electronics9111761",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3343031.3350956",
            "source": "crossref"
        },
        {
            "doi": "10.1080/10447318.2015.1064638",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2021.03.019",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neucom.2019.01.079",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2009.5459207",
            "source": "crossref"
        },
        {
            "doi": "10.4236/psych.2015.65055",
            "source": "crossref"
        },
        {
            "doi": "10.1109/OJCS.2021.3068385",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2008.4587597",
            "source": "crossref"
        },
        {
            "doi": "10.3390/app10082924",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1142/S0129065709002002",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCYB.2020.2974688",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICPR48806.2021.9412991",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00587",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JAS.2017.7510622",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3469877.3490621",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11042-017-4794-7",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW53098.2021.00148",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3127874",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.116",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-49409-8_4",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JAS.2020.1003483",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.ijhcs.2021.102744",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-54173-6_17",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACVW54805.2022.00052",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2019.00207",
            "source": "crossref"
        },
        {
            "doi": "10.5244/C.31.33",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnhum.2018.00221",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2018.2884458",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACVW50321.2020.9096944",
            "source": "crossref"
        },
        {
            "title": "YOLOv3: An incremental improvement",
            "source": "crossref"
        },
        {
            "title": "ESIM: An open event camera simulator",
            "source": "crossref"
        },
        {
            "title": "Do you feel safe with your robot? Factors influencing perceived safety in human-robot interaction based on subjective and objective measures",
            "source": "crossref"
        },
        {
            "title": "A review on facial micro-expressions analysis: Datasets, features and metrics",
            "source": "crossref"
        },
        {
            "title": "Speech emotion recognition in emotional feedback for human-robot interaction",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

- Contains data from 25 participants
- Contains 455 recordings
