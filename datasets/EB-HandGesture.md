---
{
    "name": "EB-HandGesture",
    "aliases": [],
    "year": 2024,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "Prophesee Gen3"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "Hand Pose Detection",
        "Gesture Recognition"
    ],
    "task": "Hand Gesture Recognition",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": true,
        "distribution_methods": [
            "Google Drive"
        ],
        "file_formats": [
            "HDF5"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Google Drive",
                "url": "https://drive.google.com/file/d/1s2MhiM-6P4IckIa3Sb9L0dBlzKdAIu6H/view",
                "format": "HDF5",
                "available": true
            }
        ],
        "size_gb": 3.5,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "Event Camera-Based Real-Time Gesture Recognition for Improved Robotic Guidance",
        "doi": "10.1109/IJCNN60899.2024.10650870",
        "authors": [
            "Muhammad Aitsam",
            "Sergio Davies",
            "Alessandro Di Nuovo"
        ],
        "abstract": "Recent breakthroughs in event-based vision, driven by the capabilities of high-resolution event cameras, have significantly improved human-robot interactions. Event cameras excel in managing dynamic range and motion blur, seamlessly adapting to various environmental conditions. The research presented in this paper leverages this technology to develop an intuitive robot guidance system capable of interpreting hand gestures for precise robot control. We introduce the \"EB-HandGesture\" dataset, an innovative high-resolution hand-gesture dataset used in conjunction with our network \"ConvRNN\" to demonstrate commendable accuracy of 95.7% in the interpretation task, covering six gesture types in different lighting scenarios. To validate our framework, real-life experiments were conducted with the ARI robot, confirming the effectiveness of the trained network during the various interaction processes. This research represents a substantial leap forward in ensuring safer, more reliable and more efficient human-robot collaboration in shared workspaces.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 0,
            "updated": "2025-06-30T10:52:09.202773"
        },
        {
            "source": "scholar",
            "count": 4,
            "updated": "2025-06-30T10:52:08.799556"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/10650870"
        },
        {
            "type": "github_page",
            "url": "https://github.com/aitsam12/EB-HandGesture-Recognition/blob/main/README.md"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_recordings": "9000",
        "num_classes": "6",
        "num_subjects": "5"
    },
    "bibtex": {
        "pages": "1--8",
        "year": 2024,
        "month": "jun",
        "author": "Aitsam, Muhammad and Davies, Sergio and Di Nuovo, Alessandro",
        "publisher": "IEEE",
        "booktitle": "2024 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})",
        "urldate": "2024-09-13",
        "language": "en",
        "doi": "10.1109/IJCNN60899.2024.10650870",
        "url": "https://ieeexplore.ieee.org/document/10650870/",
        "isbn": "9798350359312",
        "copyright": "https://doi.org/10.15223/policy-029",
        "title": "Event {Camera}-{Based} {Real}-{Time} {Gesture} {Recognition} for {Improved} {Robotic} {Guidance}",
        "address": "Yokohama, Japan",
        "type": "inproceedings",
        "key": "aitsam_event_2024"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/ICIP.2010.5651532",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.01258",
            "source": "crossref"
        },
        {
            "doi": "10.1021/acs.jpca.2c00806",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3529446.3529465",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-70338-1_3",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2015.08.004",
            "source": "crossref"
        },
        {
            "doi": "10.3390/electronics10202470",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.412",
            "source": "crossref"
        },
        {
            "doi": "10.3991/ijoe.v19i09.39927",
            "source": "crossref"
        },
        {
            "doi": "10.1155/2022/8777355",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.781",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCAD.2015.2474396",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2017.00309",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00437",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2021.726582",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.00215",
            "source": "crossref"
        },
        {
            "doi": "10.24963/ijcai.2021/240",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00058",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00275",
            "source": "crossref"
        },
        {
            "doi": "10.1177/0278364917691115",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2016.00594",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ITCA52113.2020.00106",
            "source": "crossref"
        },
        {
            "doi": "10.1162/neco_a_01362",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.576",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.90",
            "source": "crossref"
        },
        {
            "doi": "10.1109/iccv48922.2021.01345",
            "source": "crossref"
        },
        {
            "doi": "10.1609/aaai.v34i02.5486",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.590164",
            "source": "crossref"
        },
        {
            "title": "Evt 3.0 format \u2014 metavision sdk docs 4.3.0 documentation",
            "source": "crossref"
        },
        {
            "title": "Deep hand: How to train a cnn on 1 million hand images when your data is continuous and weakly labelled",
            "source": "crossref"
        },
        {
            "title": "I. of Electrical, and E. Engineers",
            "source": "crossref"
        },
        {
            "title": "Moddrop: adaptive multi-modal gesture recognition",
            "source": "crossref"
        },
        {
            "title": "Spinnaker 2: A 10 million core processor system for brain simulation and machine learning",
            "source": "crossref"
        },
        {
            "title": "Bio-realistic neural network implementation on loihi 2 with izhikevich neurons",
            "source": "crossref"
        },
        {
            "title": "Silkyevcam (vga) - centuryarks co., ltd",
            "source": "crossref"
        },
        {
            "title": "Learning to detect objects with a 1 megapixel event camera",
            "source": "crossref"
        },
        {
            "title": "Es-imagenet: A million event-stream classification dataset for spiking neural networks",
            "source": "crossref"
        },
        {
            "title": "Hardvs: Revisiting human activity recognition with dynamic vision sensors",
            "source": "crossref"
        },
        {
            "title": "Bullying10k: A neuromorphic dataset towards privacy-preserving bullying recognition",
            "source": "crossref"
        },
        {
            "title": "Mnist handwritten digits description and using neuromorphic hardware view project",
            "source": "crossref"
        },
        {
            "title": "Comparison of image classification techniques using caltech 101 dataset texture-based image retrieval view project mobile quranic memorization tool view project comparison of image classification techniques using caltech 101 dataset",
            "source": "crossref"
        },
        {
            "title": "Computer Vision and Pattern Recognition",
            "source": "crossref"
        },
        {
            "title": "Event-based vision: A survey",
            "source": "crossref"
        },
        {
            "title": "Training an eb classification model \u2014 metavision sdk docs 4.5.1 documentation",
            "source": "crossref"
        },
        {
            "title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \u00a10.5mb model size",
            "source": "crossref"
        },
        {
            "title": "Slowfast networks for video recognition",
            "source": "crossref"
        },
        {
            "title": "Convnet architecture search for spatiotemporal feature learning",
            "source": "crossref"
        },
        {
            "title": "X3d: Expanding architectures for efficient video recognition",
            "source": "crossref"
        },
        {
            "title": "Slayer: Spike layer error reassignment in time",
            "source": "crossref"
        },
        {
            "title": "Synaptic plasticity dynamics for deep continuous local learning (decolle)",
            "source": "crossref"
        }
    ]
}
---


### Dataset Structure 

- Contains 9000 recordings in total
- Contains 6 different classes of gestures ("wave", "point", "fist", "clap", "armroll")
- Contains data from 5 subjects




### Comparisons from paper

^cac401

| Dataset Name        | Year | Type        | Data         | Sensor        | Resolution | Sec. per Instance | Samples |
| ------------------- | ---- | ----------- | ------------ | ------------- | ---------- | ----------------- | ------- |
| [[CIFAR10-DVS]]     | 2017 |             | images       | DAVIS128      | 128x128    | 1.2s              | 10k     |
| [[MNIST-DVS]]       | 2013 | reproduced  | digit images | DAVIS128      | 128x128    | 2-3s              | 30k     |
| [[N-MNIST]]         | 2015 | reproduced  | digit images | ATIS          | 28×28      | 0.3s              | 70k     |
| [[N-CALTECH101]]    | 2015 | reproduced  | images       | ATIS          | 302×245    | 0.3s              | 8.7k    |
| [[ES-ImageNet]]     | 2021 | converted   | images       |               | 224x224    |                   | 1.3mil  |
| [[N-ImageNet]]      | 2021 | reproduced  | images       | Samsung Gen3  | 480x640    | -                 | 1.7mil  |
| [[HARDVS]]          | 2022 | event-based | action       | DAVIS346      | 346×260    | 5s                | 100k    |
| [[DailyAction-DVS]] | 2021 | event-based | action       | DAVIS346      | 346x260    | 5s                | 1.4k    |
| [[Bullying10K]]     | 2023 | event-based | action       | DAVIS346      | 346×260    | 2-20s             | 10k     |
| [[ASL-DVS]]         | 2019 | event-basec | hand action  | DAVIS240      | 240x180    | 0.1s              | 100k    |
| [[NavGesture]]      | 2020 | event-based | hand action  | ATIS          | 302x245    |                   | 1.3k    |
| [[DVS-GESTURE]]     | 2017 | event-based | hand action  | DAVIS 128     | 128x128    | 6s                | 1.3k    |
| [[EB-HandGesture]]  | 2024 | event-based | hand action  | SilkyCam Gen3 | 640х480    | 0.5s              | 1.5k    |

^1aa4f5