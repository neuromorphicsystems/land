---
{
    "name": "DVSACT16",
    "aliases": [],
    "year": 2016,
    "modalities": [
        "Vision"
    ],
    "sensors": [
        "DAVIS240"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Monitor Conversion"
    ],
    "description": "Object and Action Recognition",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": false,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [
            "Zenodo"
        ],
        "file_formats": [
            "HDF5"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Zenodo",
                "url": "https://zenodo.org/records/4807304",
                "format": "HDF5",
                "available": true
            }
        ],
        "size_gb": 26.0,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "DVS Benchmark Datasets for Object Tracking, Action Recognition, and Object Recognition",
        "doi": "10.3389/fnins.2016.00405",
        "authors": [
            "Yuhuang Hu",
            "Hongjie Liu",
            "Michael Pfeiffer",
            "Tobi Delbruck"
        ],
        "abstract": "No abstract",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 92,
            "updated": "2025-07-02T14:07:51.368504"
        },
        {
            "source": "scholar",
            "count": 170,
            "updated": "2025-07-02T14:07:50.920232"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00405/full"
        },
        {
            "type": "project_page",
            "url": "https://docs.google.com/document/d/1m4gAHPkPIzVvhHirtSjzUJrKiD5uU2I76hTqXhk8CxI/edit#heading=h.m1in3r34wdsc"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "stereo": false
    },
    "bibtex": {
        "year": 2016,
        "month": "aug",
        "author": "Hu, Yuhuang and Liu, Hongjie and Pfeiffer, Michael and Delbruck, Tobi",
        "journal": "Frontiers in Neuroscience",
        "urldate": "2023-09-05",
        "language": "en",
        "doi": "10.3389/fnins.2016.00405",
        "url": "http://journal.frontiersin.org/Article/10.3389/fnins.2016.00405/abstract",
        "issn": "1662-453X",
        "volume": "10",
        "title": "{DVS} {Benchmark} {Datasets} for {Object} {Tracking}, {Action} {Recognition}, and {Object} {Recognition}",
        "type": "article",
        "key": "hu_dvs_2016"
    },
    "referenced_papers": [
        {
            "doi": "10.3389/fnins.2016.00049",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2012.02.022",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-014-0788-3",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2516982",
            "source": "crossref"
        },
        {
            "doi": "10.1038/nature14539",
            "source": "crossref"
        },
        {
            "doi": "10.1109/5.726791",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2013.00178",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00437",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2013.71",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JPROC.2014.2346153",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s00138-012-0450-4",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2011.2180025",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2016.00176",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00481",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00374",
            "source": "crossref"
        },
        {
            "title": "A dataset for visual navigation with neuromorphic methods",
            "source": "crossref"
        },
        {
            "title": "A 240 \u00d7 180 10mw 12\u03bcs latency sparse-output vision sensor for mobile applications",
            "source": "crossref"
        },
        {
            "title": "Extraction of temporally correlated features from dynamic vision sensors with spike-timing-dependent plasticity",
            "source": "crossref"
        },
        {
            "title": "A 240 \u00d7 180 130 db 3\u03bcs latency global shutter spatiotemporal vision sensor",
            "source": "crossref"
        },
        {
            "title": "Spiking deep convolutional neural networks for energy-efficient object recognition",
            "source": "crossref"
        },
        {
            "title": "Imagenet: a large-scale hierarchical image database",
            "source": "crossref"
        },
        {
            "title": "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing",
            "source": "crossref"
        },
        {
            "title": "A novel performance evaluation methodology for single-target trackers",
            "source": "crossref"
        },
        {
            "title": "Imagenet classification with deep convolutional neural networks",
            "source": "crossref"
        },
        {
            "title": "Deep learning",
            "source": "crossref"
        },
        {
            "title": "Gradient-based learning applied to document recognition",
            "source": "crossref"
        },
        {
            "title": "A 128 \u00d7 128 120 db 15 \u03bcs latency asynchronous temporal contrast vision sensor",
            "source": "crossref"
        },
        {
            "title": "Real-time classification and sensor fusion with a spiking deep belief network",
            "source": "crossref"
        },
        {
            "title": "Converting static image datasets to spiking neuromorphic datasets using saccades",
            "source": "crossref"
        },
        {
            "title": "Mapping from frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing\u2013application to feedforward convnets",
            "source": "crossref"
        },
        {
            "title": "Retinomorphic event-based vision sensors: bioinspired cameras with spiking output",
            "source": "crossref"
        },
        {
            "title": "Recognizing 50 human action categories of web videos",
            "source": "crossref"
        },
        {
            "title": "Asynchronous event-based binocular stereo matching",
            "source": "crossref"
        },
        {
            "title": "Evaluation of event-based algorithms for optical flow with ground-truth from inertial measurement sensor",
            "source": "crossref"
        },
        {
            "title": "Poker-DVS and MNIST-DVS their history, how they were made, and other details",
            "source": "crossref"
        },
        {
            "title": "Benchmarking neuromorphic vision: lessons learnt from computer vision",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

This data report summarizes a new benchmark dataset in which we converted established visual video benchmarks for object tracking, action recognition and object recognition into spiking neuromorphic datasets, recorded with the DVS output (Lichtsteiner et al., 2008) of a DAVIS camera (Berner et al., 2013; Brandli et al., 2014). This report presents our approach for sensor calibration and capture of frame-based videos into neuromorphic vision datasets with minimal human intervention. We converted four widely used dynamic datasets: the VOT Challenge 2015 Dataset (Kristan et al., 2016), TrackingDataset3, the UCF-50 Action Recognition Dataset (Reddy and Shah, 2012), and the Caltech-256 Object Category Dataset (Griffin et al., 2006).
