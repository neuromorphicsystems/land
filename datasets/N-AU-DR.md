---
{
    "name": "N-AU-DR",
    "aliases": [],
    "year": 2022,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DAVIS240"
    ],
    "other_sensors": [],
    "category": "Robotic and Moving Vehicle Datasets",
    "subcategory": [
        "Drone Racing",
        "Drone Detection"
    ],
    "task": "Navigation for drone racing",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": true,
        "has_ground_truth": false,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [
            "Google Drive"
        ],
        "file_formats": [
            "Binary"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Google Drive",
                "url": "https://drive.google.com/drive/folders/1InoQCxsV5SEjSLCI9hsjjMPBX8-QMu_H",
                "format": "Binary",
                "available": false
            }
        ],
        "size_gb": 33.0,
        "size_type": "Uncompressed"
    },
    "paper": {
        "title": "Event-based Navigation for Autonomous Drone Racing with Sparse Gated Recurrent Network",
        "doi": "10.23919/ECC55457.2022.9838538",
        "authors": [
            "Kristoffer Fogh Andersen",
            "Huy Xuan Pham",
            "Halil Ibrahim Ugurlu",
            "Erdal Kayacan"
        ],
        "abstract": "Event-based vision has already revolutionized the perception task for robots by promising faster response, lower energy consumption, and lower bandwidth without introducing motion blur. In this work, a novel deep learning method based on gated recurrent units utilizing sparse convolutions for detecting gates in a race track is proposed using event-based vision for the autonomous drone racing problem. We demonstrate the efficiency and efficacy of the perception pipeline on a real robot platform that can safely navigate a typical autonomous drone racing track in real-time. Throughout the experiments, we show that the event-based vision with the proposed gated recurrent unit and pretrained models on simulated event data significantly improve the gate detection precision. Furthermore, an event-based drone racing dataset 1 1 The code and data will be available at https://github.com/open-airlab/neuromorphic_au_drone_racing.git consisting of both simulated and real data sequences is publicly released.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 6,
            "updated": "2025-06-21T17:19:07.540689"
        },
        {
            "source": "scholar",
            "count": 8,
            "updated": "2025-06-21T17:19:06.669702"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2204.02120"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/9838538"
        },
        {
            "type": "github_page",
            "url": "https://github.com/open-airlab/neuromorphic_au_drone_racing"
        }
    ],
    "full_name": "Neuromorphic AU Drone Racing (N-AU-DR)",
    "additional_metadata": {
        "stereo": false
    },
    "bibtex": {
        "pages": "1342--1348",
        "year": 2022,
        "month": "jul",
        "author": "Andersen, Kristoffer Fogh and Pham, Huy Xuan and Ugurlu, Halil Ibrahim and Kayacan, Erdal",
        "publisher": "IEEE",
        "booktitle": "2022 {European} {Control} {Conference} ({ECC})",
        "urldate": "2024-12-15",
        "language": "en",
        "doi": "10.23919/ECC55457.2022.9838538",
        "url": "https://ieeexplore.ieee.org/document/9838538/",
        "isbn": "978-3-907144-07-7",
        "copyright": "https://doi.org/10.15223/policy-029",
        "title": "Event-based {Navigation} for {Autonomous} {Drone} {Racing} with {Sparse} {Gated} {Recurrent} {Network}",
        "address": "London, United Kingdom",
        "type": "inproceedings",
        "key": "andersen_event-based_2022"
    },
    "referenced_papers": [
        {
            "doi": "10.3115/v1/D14-1179",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICNN.1994.374611",
            "source": "crossref"
        },
        {
            "doi": "10.1162/neco.1997.9.8.1735",
            "source": "crossref"
        },
        {
            "doi": "10.1016/B978-0-32-385787-1.00020-8",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS51168.2021.9636207",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IJCNN48605.2020.9206943",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2020.2965911",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00186",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58598-3_25",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ECC.2015.7330991",
            "source": "crossref"
        },
        {
            "doi": "10.1109/DASC50938.2020.9256492",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ECC.2015.7330695",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA40945.2020.9196845",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.91",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2021.3057003",
            "source": "crossref"
        },
        {
            "doi": "10.23919/ECC.2009.7074867",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IJCNN48605.2020.9207490",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00573",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58565-5_9",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2020.3002480",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00437",
            "source": "crossref"
        },
        {
            "title": "Adam: A method for stochastic optimization",
            "source": "crossref"
        },
        {
            "title": "ESIM: an open event camera simulator",
            "source": "crossref"
        },
        {
            "title": "Alphapilot: Autonomous drone racing",
            "source": "crossref"
        },
        {
            "title": "End-to-end motion planning of quadrotors using deep reinforcement learning",
            "source": "crossref"
        },
        {
            "title": "Context-dependent anomaly detection for low altitude traffic surveillance",
            "source": "crossref"
        },
        {
            "title": "Event-based vision: A survey",
            "source": "crossref"
        },
        {
            "title": "You only look once: Unified, real-time object detection",
            "source": "crossref"
        },
        {
            "title": "Representation learning for event-based visuomotor policies",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

- The overall structure of the dataset is aligned with how the [Gen1 Automotive dataset](https://www.prophesee.ai/2020/01/24/prophesee-gen1-automotive-detection-dataset/) is structured, and all tools compatible with this dataset can also be used for our dataset.

- The dataset is organized into .dat files each containing 60 seconds of event data, either simulated or real. Each dat file is a binary file in which events are encoded using 4 bytes (unsigned int32) for the timestamps and 4 bytes (unsigned int32) for the data

- The data is composed of 14 bits for the x position, 14 bits for the y position and 1 bit for the polarity (encoded as -1/1).

- For N-AU-DR-sim, the .dat filenames identify the date, timestamp and the simulated contrast threshold used. For example: 29-04-2021_15-21-46_1619702295_0.3_td.dat.

- For N-AU-DR-real train split, the .dat filenames identify the date, timestamp, number of drone rotations and drone velocity. For example: 18-06-2021_10-57-20_1624006640_6rot_075ms_td.dat

- For N-AU-DR-real validation split, the .dat filenames identify the date, timestamp, environment lighting and whether gates have the same placement as in the training data. For example: 10-07-2021_13-58-01_1625918281_50light_gatemoved_td.dat

- For each .dat file there is a corresponding .npy file containing the annotated bounding boxes of gates. Each bounding box consist of `x` abscissa of the top left corner in pixels, `y` ordinate of the top left corner in pixels, `w` width of the boxes in pixel, `h` height of the boxes in pixel, `ts` timestamp of the box in the sequence in microseconds, `class_id` 0 for gates.
