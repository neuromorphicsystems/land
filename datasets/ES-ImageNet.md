---
{
    "name": "ES-ImageNet",
    "aliases": [],
    "year": 2021,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "Custom Simulator"
    ],
    "other_sensors": [],
    "category": "Benchmarking, SNN Training Task, and SNN Training",
    "subcategory": [
        "Monitor Conversion",
        "Classification Datasets"
    ],
    "task": "Bechmarking dataset for SNNs",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": false,
        "has_simulated_data": true,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [
            "Other"
        ],
        "file_formats": [
            "Numpy"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Other",
                "url": "https://cloud.tsinghua.edu.cn/d/94873ab4ec2a4eb497b3/",
                "format": "Numpy",
                "available": true
            }
        ],
        "size_gb": 100.0,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks",
        "doi": "10.3389/fnins.2021.726582",
        "authors": [
            "Yihan Lin",
            "Wei Ding",
            "Shaohua Qiang",
            "Lei Deng",
            "Guoqi Li"
        ],
        "abstract": "With event-driven algorithms, especially spiking neural networks (SNNs), achieving continuous improvement in neuromorphic vision processing, a more challenging event-stream dataset is urgently needed. However, it is well-known that creating an ES-dataset is a time-consuming and costly task with neuromorphic cameras like dynamic vision sensors (DVS). In this work, we propose a fast and effective algorithm termed Omnidirectional Discrete Gradient (ODG) to convert the popular computer vision dataset ILSVRC2012 into its event-stream (ES) version, generating about 1,300,000 frame-based images into ES-samples in 1,000 categories. In this way, we propose an ES-dataset called ES-ImageNet, which is dozens of times larger than other neuromorphic classification datasets at present and completely generated by the software. The ODG algorithm implements image motion to generate local value changes with discrete gradient information in different directions, providing a low-cost and high-speed method for converting frame-based images into event streams, along with Edge-Integral to reconstruct the high-quality images from event streams. Furthermore, we analyze the statistics of ES-ImageNet in multiple ways, and a performance benchmark of the dataset is also provided using both famous deep neural network algorithms and spiking neural network algorithms. We believe that this work shall provide a new large-scale benchmark dataset for SNNs and neuromorphic vision.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 23,
            "updated": "2025-07-03T09:31:05.001445"
        },
        {
            "source": "scholar",
            "count": 45,
            "updated": "2025-07-03T09:31:04.664417"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2110.12211"
        },
        {
            "type": "paper",
            "url": "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2021.726582/full"
        },
        {
            "type": "github_page",
            "url": "https://github.com/lyh983012/ES-imagenet-master"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "source_dataset": "LSVRC2012",
        "stereo": false
    },
    "bibtex": {
        "pages": "726582",
        "year": 2021,
        "month": "nov",
        "author": "Lin, Yihan and Ding, Wei and Qiang, Shaohua and Deng, Lei and Li, Guoqi",
        "journal": "Frontiers in Neuroscience",
        "urldate": "2024-04-13",
        "language": "en",
        "doi": "10.3389/fnins.2021.726582",
        "url": "https://www.frontiersin.org/articles/10.3389/fnins.2021.726582/full",
        "shorttitle": "{ES}-{ImageNet}",
        "issn": "1662-453X",
        "volume": "15",
        "title": "{ES}-{ImageNet}: {A} {Million} {Event}-{Stream} {Classification} {Dataset} for {Spiking} {Neural} {Networks}",
        "type": "article",
        "key": "lin_es-imagenet_2021"
    },
    "referenced_papers": [
        {
            "doi": "10.1113/jphysiol.1926.sp002281",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCAD.2015.2474396",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2013.2273537",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2013.03.006",
            "source": "crossref"
        },
        {
            "doi": "10.1109/MM.2018.112130359",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2019.09.005",
            "source": "crossref"
        },
        {
            "doi": "10.1038/scientificamerican0374-34",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2016.00405",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2012.59",
            "source": "crossref"
        },
        {
            "doi": "10.1109/36.387576",
            "source": "crossref"
        },
        {
            "doi": "10.1109/5.726791",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2017.00309",
            "source": "crossref"
        },
        {
            "doi": "10.1016/S0893-6080(97)00011-7",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnbot.2019.00038",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00437",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2019.2903179",
            "source": "crossref"
        },
        {
            "doi": "10.1038/s41586-019-1424-8",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2013.71",
            "source": "crossref"
        },
        {
            "doi": "10.1038/s41586-019-1677-2",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-015-0816-y",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.neunet.2007.12.009",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TC.2016.2574353",
            "source": "crossref"
        },
        {
            "doi": "10.1109/FG47880.2020.00069",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2021.3054886",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2018.2876179",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2015.2388544",
            "source": "crossref"
        },
        {
            "title": "The impulses produced by sensory nerve-endings: part II. the response of a single end-organ",
            "source": "crossref"
        },
        {
            "title": "Truenorth: design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip",
            "source": "crossref"
        },
        {
            "title": "A low power, fully event-based gesture recognition system",
            "source": "crossref"
        },
        {
            "title": "Simultaneous optical flow and intensity estimation from an event camera",
            "source": "crossref"
        },
        {
            "title": "Event-based visual flow",
            "source": "crossref"
        },
        {
            "title": "Pix2nvs: parameterized conversion of pixel-domain video frames to neuromorphic vision streams",
            "source": "crossref"
        },
        {
            "title": "A 240 \u00d7 180 130 db 3 \u03bcs latency global shutter spatiotemporal vision sensor",
            "source": "crossref"
        },
        {
            "title": "N-rod: a neuromorphic dataset for synthetic-to-real domain adaptation",
            "source": "crossref"
        },
        {
            "title": "Event-based 3d reconstruction from neuromorphic retinas",
            "source": "crossref"
        },
        {
            "title": "Loihi: a neuromorphic many-core processor with on-chip learning",
            "source": "crossref"
        },
        {
            "title": "A large scale event-based detection dataset for automotive",
            "source": "crossref"
        },
        {
            "title": "Imagenet: a large-scale hierarchical image database",
            "source": "crossref"
        },
        {
            "title": "Rethinking the performance comparison between snns and anns",
            "source": "crossref"
        },
        {
            "title": "The neural basis of visually guided behavior",
            "source": "crossref"
        },
        {
            "title": "Video to events: recycling video datasets for event cameras",
            "source": "crossref"
        },
        {
            "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?",
            "source": "crossref"
        },
        {
            "title": "Deep residual learning for image recognition",
            "source": "crossref"
        },
        {
            "title": "Dvs benchmark datasets for object tracking, action recognition, and object recognition",
            "source": "crossref"
        },
        {
            "title": "Batch normalization: accelerating deep network training by reducing internal covariate shift",
            "source": "crossref"
        },
        {
            "title": "3d convolutional neural networks for human action recognition",
            "source": "crossref"
        },
        {
            "title": "Real-time 3d reconstruction and 6-dof tracking with an event camera",
            "source": "crossref"
        },
        {
            "title": "Adam: a method for stochastic optimization",
            "source": "crossref"
        },
        {
            "title": "Imagenet classification with deep convolutional neural networks",
            "source": "crossref"
        },
        {
            "title": "Refining image segmentation by integration of edge and region data",
            "source": "crossref"
        },
        {
            "title": "Gradient-based learning applied to document recognition",
            "source": "crossref"
        },
        {
            "title": "Cifar10-dvs: an event-stream dataset for object classification",
            "source": "crossref"
        },
        {
            "title": "Networks of spiking neurons: the third generation of neural network models",
            "source": "crossref"
        },
        {
            "title": "Neuromorphic benchmark datasets for pedestrian detection, action recognition, and fall detection",
            "source": "crossref"
        },
        {
            "title": "Pred18: dataset and further experiments with davis event camera in predator-prey robot chasing",
            "source": "crossref"
        },
        {
            "title": "Ms marco: a human-generated machine reading comprehension dataset",
            "source": "crossref"
        },
        {
            "title": "Confident learning: estimating uncertainty in dataset labels",
            "source": "crossref"
        },
        {
            "title": "Converting static image datasets to spiking neuromorphic datasets using saccades",
            "source": "crossref"
        },
        {
            "title": "Unsupervised learning of a hierarchical spiking neural network for optical flow estimation: from events to global motion perception",
            "source": "crossref"
        },
        {
            "title": "Pytorch: an imperative style, high-performance deep learning library",
            "source": "crossref"
        },
        {
            "title": "Towards artificial general intelligence with hybrid tianjic chip architecture",
            "source": "crossref"
        },
        {
            "title": "Mapping from frame-driven to frame-free event-driven vision systems by low-rate coding and coincidence processing\u2013application to feedforward convnets",
            "source": "crossref"
        },
        {
            "title": "Random features for large-scale kernel machines",
            "source": "crossref"
        },
        {
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "source": "crossref"
        },
        {
            "title": "Towards spike-based machine intelligence with neuromorphic computing",
            "source": "crossref"
        },
        {
            "title": "ImageNet large scale visual recognition challenge",
            "source": "crossref"
        },
        {
            "title": "Compact hardware liquid state machines on fpga for real-time speech recognition",
            "source": "crossref"
        },
        {
            "title": "Dadiannao: a neural network supercomputer",
            "source": "crossref"
        },
        {
            "title": "Introduction and analysis of an event-based sign language dataset",
            "source": "crossref"
        },
        {
            "title": "Event-stream representation for human gaits identification using deep neural networks",
            "source": "crossref"
        },
        {
            "title": "l1 -norm batch normalization for efficient training of deep neural networks",
            "source": "crossref"
        },
        {
            "title": "Direct training for spiking neural networks: faster, larger, better",
            "source": "crossref"
        },
        {
            "title": "Liaf-net: leaky integrate and analog fire network for lightweight and efficient spatiotemporal information processing",
            "source": "crossref"
        },
        {
            "title": "Dashnet: a hybrid artificial and spiking neural network for high-speed object tracking",
            "source": "crossref"
        },
        {
            "title": "A digital liquid state machine with biologically inspired learning and its application to speech recognition",
            "source": "crossref"
        }
    ]
}
---


### Dataset Structure 

- Converted from the LSVRC2012 dataset


### Comparison Table

| Name                                       | Generating Speed<sup>a</sup>       | Resolution           | # of Samples                            | # Classes | Type      |
|--------------------------------------------|------------------------------------|----------------------|-----------------------------------------|-----------|-----------|
| POKER-DVS (Prez-Carrasco et al., 2013)     | –                                  | 32×32                | 131                                     | 4         | Classify  |
| N-MNIST (Orchard et al., 2015)             | 300 ms/sample                      | 28×28                | 60,000 training + 10,000 test           | 10        | Classify  |
| DVS-Caltech101 (Orchard et al., 2015)      | 300 ms/sample                      | 302×245 (avg)        | 8,709                                   | 100       | Classify  |
| DVS-UCF-50 (Hu et al., 2016)               | 6,800 ms/sample                    | 240×180              | 6,676                                   | 50        | Classify  |
| DVS-Caltech-256 (Hu et al., 2016)          | 1,010 ms/sample                    | 240×180              | 30,607                                  | 257       | Classify  |
| DVS-VOT-2015 (Hu et al., 2016)             | 30 FPS, 20.70 s/sample             | 240×180              | 67                                      | –         | Track     |
| DVS-CIFAR10 (Li et al., 2017)              | 300 ms/sample                      | 512×512              | 10,000                                  | 10        | Classify  |
| DVS-Gesture (Amir et al., 2017)            | 6 s/sample                         | 128×128              | 1,342                                   | 11        | Classify  |
| Pred-18 (Moeys et al., 2018)               | 15 FPS                             | 240×180              | 1.25 h (67.5k frames)                   | 2         | Detect    |
| Action Recognition (Miao et al., 2019)     | 5 s/sample                         | 346×260              | 450                                     | 10        | Classify  |
| 1Mpx Detection (de Tournemire et al., 2020)| 60 s/sample                        | 304×240              | 14.65 h, 255,781 objects                | 2         | Detect    |
| SL-ANIMALS-DVS (Vasudevan et al., 2020)    | –                                  | 128×128              | 1,102                                   | 10        | Classify  |
| DVS-Gait-Day/Night (Wang et al., 2020)     | 3–4 s/sample                       | 128×128              | 4,000                                   | 20        | Classify  |
| N-ROD (Cannici et al., 2021)               | 6.6 s/sample                       | 256×256              | 41,877                                  | 51        | Classify  |
| ES-ImageNet                                | 29.47 ms/sample<sup>b</sup>        | 224×224<sup>c</sup>  | 1,257,035 training + 49,881 test        | 1,000     | Classify  |