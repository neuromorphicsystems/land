---
{
    "name": "EvCSLR",
    "aliases": [],
    "year": 2024,
    "modalities": [
        "Vision"
    ],
    "sensors": [
        "DAVIS346"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Sign Language",
        "Hand Pose Detection",
        "Colour Sensor"
    ],
    "description": "Sign Language Dataset",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [
            "Baidu"
        ],
        "file_formats": [
            "Numpy"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Baidu",
                "url": "https://pan.baidu.com/s/1JM_IwtiB7Obl7G18o3RnMQ?pwd=j6vb",
                "format": "Numpy",
                "available": true
            }
        ],
        "size_gb": 23.66,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "EvCSLR: Event-Guided Continuous Sign Language Recognition and Benchmark",
        "doi": "10.1109/TMM.2024.3521750",
        "authors": [
            "Yu Jiang",
            "Yuehang Wang",
            "Siqi Li",
            "Yongji Zhang",
            "Qianren Guo",
            "Qi Chu",
            "Yue Gao"
        ],
        "abstract": "Classical continuous sign language recognition (CSLR) suffers from some main challenges in real-world scenarios: accurate inter-frame movement trajectories may fail to be captured by traditional RGB cameras due to the motion blur, and valid information may be insufficient under low-illumination scenarios. In this paper, we for the first time leverage an event camera to overcome the above-mentioned challenges. Event cameras are bio-inspired vision sensors that could efficiently record high-speed sign language movements under low-illumination scenarios and capture human information while eliminating redundant background interference. To fully exploit the benefits of the event camera for CSLR, we propose a novel event-guided multi-modal CSLR framework, which could achieve significant performance under complex scenarios. Specifically, a time redundancy correction (TRCorr) module is proposed to rectify redundant information in the temporal sequences, directing the model to focus on distinctive features. A multi-modal cross-attention interaction (MCAI) module is proposed to facilitate information fusion between events and frame domains. Furthermore, we construct the first event-based CSLR dataset, named EvCSLR, which will be released as the first event-based CSLR benchmark. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on EvCSLR and PHOENIX-2014 T datasets.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 0,
            "updated": "2025-09-14T23:28:06.405641"
        },
        {
            "source": "scholar",
            "count": 4,
            "updated": "2025-09-14T23:28:06.993829"
        }
    ],
    "links": [
        {
            "type": "github_page",
            "url": "https://github.com/diamondxx/EvCSLR"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/abstract/document/10814091"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_subjects": "18",
        "language": "Chinese"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/TMM.2021.3087006",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2021.3109665",
            "source": "crossref"
        },
        {
            "doi": "10.1109/34.735811",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCYB.2013.2265337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.01090",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.01134",
            "source": "crossref"
        },
        {
            "doi": "10.24963/ijcai.2023/85",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2022.3223260",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2023.3268368",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TIP.2019.2941267",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2015.09.013",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00812",
            "source": "crossref"
        },
        {
            "doi": "10.1609/aaai.v32i1.11903",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR46437.2021.00137",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2005.112",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR42600.2020.00043",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICSMC.1997.625742",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCVW60793.2023.00345",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2023.3321502",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52729.2023.01430",
            "source": "crossref"
        },
        {
            "doi": "10.1016/S0031-3203(04)00165-7",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.364",
            "source": "crossref"
        },
        {
            "doi": "10.5244/C.30.136",
            "source": "crossref"
        },
        {
            "doi": "10.1145/1143844.1143891",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2024.3377095",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2021.3059098",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV51070.2023.01890",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52729.2023.01037",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2021.3070438",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52688.2022.00507",
            "source": "crossref"
        },
        {
            "doi": "10.1609/aaai.v37i1.25164",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52729.2023.00249",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV45572.2020.9093512",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCSVT.2018.2870740",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2019.2963386",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2019.00209",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-031-19830-4_20",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMM.2024.3380255",
            "source": "crossref"
        },
        {
            "doi": "10.24963/ijcai.2021/240",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2023.3300741",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00108",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00745",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.90",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-031-19833-5_30",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.01111",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52688.2022.00564",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW53098.2021.00144",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.74",
            "source": "crossref"
        },
        {
            "title": "Orientation histograms for hand gesture recognition",
            "source": "crossref"
        },
        {
            "title": "Ms-asl: A large-scale data set and benchmark for understanding american sign language",
            "source": "crossref"
        },
        {
            "title": "PyTorch: An imperative style, high-performance deep learning library",
            "source": "crossref"
        },
        {
            "title": "Adam: A method for stochastic optimization",
            "source": "crossref"
        },
        {
            "title": "Accelerating t-SNE using tree-based algorithms",
            "source": "crossref"
        }
    ],
    "bibtex": {
        "pages": "1349\u20131361",
        "year": 2025,
        "author": "Jiang, Yu and Wang, Yuehang and Li, Siqi and Zhang, Yongji and Guo, Qianren and Chu, Qi and Gao, Yue",
        "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
        "journal": "IEEE Transactions on Multimedia",
        "doi": "10.1109/tmm.2024.3521750",
        "url": "http://dx.doi.org/10.1109/TMM.2024.3521750",
        "issn": "1941-0077",
        "volume": "27",
        "title": "EvCSLR: Event-Guided Continuous Sign Language Recognition and Benchmark",
        "type": "article",
        "key": "Jiang_2025"
    }
}
---

# Dataset Description