---
{
    "name": "MICC-Event",
    "aliases": [],
    "year": 2021,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "Prophesee Gen3"
    ],
    "other_sensors": [],
    "category": "Intensity Reconstruction, Optical Flow, and Frame Fusion",
    "subcategory": [
        "Frame Reconstruction"
    ],
    "task": "Event to frame conversions",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [],
        "availability_comment": "Paper states that dataset will be released on publication but no dataset link was provided.",
        "dataset_links": []
    },
    "paper": {
        "title": "Temporal Binary Representation for Event-Based Action Recognition",
        "doi": "10.1109/ICPR48806.2021.9412991",
        "authors": [
            "Simone Undri Innocenti",
            "Federico Becattini",
            "Federico Pernici",
            "Alberto Del Bimbo"
        ],
        "abstract": "In this paper we present an event aggregation strategy to convert the output of an event camera into frames processable by traditional Computer Vision algorithms. The proposed method first generates sequences of intermediate binary representations, which are then losslessly transformed into a compact format by simply applying a binary-to-decimal conversion. This strategy allows us to encode temporal information directly into pixel values, which are then interpreted by deep learning models. We apply our strategy, called Temporal Binary Representation, to the task of Gesture Recognition, obtaining state of the art results on the popular DVS128 Gesture Dataset. To underline the effectiveness of the proposed method compared to existing ones, we also collect an extension of the dataset under more challenging conditions on which to perform experiments.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 51,
            "updated": "2025-07-14T09:51:25.862743"
        },
        {
            "source": "scholar",
            "count": 96,
            "updated": "2025-07-14T09:51:25.601331"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2010.08946"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/9412991"
        },
        {
            "type": "github_page",
            "url": "https://github.com/francescoareoluci/tbr-event-object-detection"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_subjects": "7",
        "num_recordings": "231",
        "stereo": false
    },
    "bibtex": {
        "doi": "10.1109/ICPR48806.2021.9412991",
        "keywords": "Deep learning;Computer vision;Computational modeling;Gesture recognition;Benchmark testing;Cameras;Task analysis",
        "pages": "10426-10432",
        "number": "",
        "volume": "",
        "year": 2021,
        "title": "Temporal Binary Representation for Event-Based Action Recognition",
        "booktitle": "2020 25th International Conference on Pattern Recognition (ICPR)",
        "author": "Innocenti, Simone Undri and Becattini, Federico and Pernici, Federico and Del Bimbo, Alberto",
        "type": "inproceedings",
        "key": "9412991"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/CVPR.2017.502",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3065386",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2013.00178",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC.2006.1696265",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2015.7298594",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.293",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2016.211",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.155",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2019.102886",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00139",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00560",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2015.7298676",
            "source": "crossref"
        },
        {
            "doi": "10.5244/C.30.58",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2017.393",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV.2019.00199",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58565-5_9",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.781",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00424",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2019.00209",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.imavis.2009.11.014",
            "source": "crossref"
        },
        {
            "doi": "10.1145/1922649.1922653",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnbot.2019.00038",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2013.65",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2019.00207",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3402447",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TSMCC.2007.893280",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-49409-8_4",
            "source": "crossref"
        },
        {
            "doi": "10.1007/11872320_24",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.ergon.2017.02.004",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00275",
            "source": "crossref"
        },
        {
            "title": "Real-time classification and sensor fusion with a spiking deep belief network",
            "source": "crossref"
        },
        {
            "title": "Multi-region two-stream r-cnn for action detection",
            "source": "crossref"
        },
        {
            "title": "Slayer: Spike layer error reassignment in time",
            "source": "crossref"
        },
        {
            "title": "Am i done? predicting action progress in videos",
            "source": "crossref"
        }
    ]
}
---


### Dataset Structure 

The recorded actions match the 11 classes of the DVS-Gesture dataset but are performed under more challenging conditions. In particular, the actors were asked to perform the actions at different speeds, in order to demonstrate the capacity of event cameras to capture high speed movements. In addition the actions have been recorded at different scales and camera orientations and also under uneven illumination which is likely to cast shadows on the body and the surroundings, generating spurious events. 

The dataset was recorded by 7 different actors of different age, height and gender for a total of 231 videos. All the videos are used for testing, still using the DVS128 Gesture Dataset as training set.