---
{
    "name": "JRC-INVISIONS",
    "aliases": [],
    "year": 2025,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "Prophesee Gen4",
        "DAVIS346"
    ],
    "other_sensors": [],
    "category": "Benchmarking, SNN Training Task, and SNN Training",
    "tags": [
        "Benchmarking"
    ],
    "description": "Benchmarking event cameras",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": true,
        "distribution_methods": [
            "Direct Download"
        ],
        "file_formats": [
            "HDF5"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Direct Download",
                "url": "https://data.jrc.ec.europa.eu/dataset/2286c9e8-84ec-4301-a3fe-127fb1a209b9",
                "format": "HDF5",
                "available": true
            }
        ],
        "size_gb": 544.0,
        "size_type": "Uncompressed"
    },
    "paper": {
        "title": "A Systematic Parametric Campaign to Benchmark Event Cameras in Computer Vision Tasks",
        "doi": "10.3390/electronics14132603",
        "authors": [
            "Dario Cazzato",
            "Graziano Renaldi",
            "Flavio Bono"
        ],
        "abstract": "The dynamic vision sensor (DVS), or event camera, is emerging as a successful sensing solution for many application fields. While state-of-the-art datasets for event-based vision are well-structured and suitable for the designed goals, they often rely on simulated data or are recorded in loosely controlled conditions, thereby making it challenging to understand the sensor response to varying camera parameters and illumination conditions. To address this knowledge gap, this work introduces the JRC INVISIONS Neuromorphic Sensors Parametric Tests dataset, an extensive collection of event-based data specifically acquired in controlled scenarios that systematically vary bias settings and environmental factors, enabling rigorous evaluation of sensor performance, robustness, and artifacts under realistic conditions that existing datasets lack. The dataset is composed of 2156 scenes recorded with two different off-the-shelf event cameras, eventually paired with a frame camera across three different controlled scenarios: moving targets, mechanical vibrations, and rotation speed estimation; the inclusion of ground truth enables the evaluation of standard computer vision tasks. The proposed manuscript is complemented by an experimental analysis of sensor performance under varying speeds and illumination, event statistics, and acquisition artifacts such as event loss and motion-induced distortions due to line-based readout. The dataset is publicly available and, to the best of our knowledge, represents the first dataset of its kind in the literature, providing a valuable resource for the research community to advance the development of event-based vision systems and applications.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 0,
            "updated": "2025-07-15T09:14:59.831255"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://www.mdpi.com/2079-9292/14/13/2603#B10-electronics-14-02603"
        }
    ],
    "full_name": "JRC INVISIONS Neuromorphic Sensors Parametric Tests Dataset",
    "additional_metadata": {
        "stereo": false
    },
    "bibtex": {
        "doi": "10.3390/electronics14132603",
        "issn": "2079-9292",
        "url": "https://www.mdpi.com/2079-9292/14/13/2603",
        "article-number": "2603",
        "number": "13",
        "year": 2025,
        "volume": "14",
        "journal": "Electronics",
        "title": "A Systematic Parametric Campaign to Benchmark Event Cameras in Computer Vision Tasks",
        "author": "Cazzato, Dario and Renaldi, Graziano and Bono, Flavio",
        "type": "article",
        "key": "electronics14132603"
    },
    "referenced_papers": [
        {
            "doi": "10.1145/3656469",
            "source": "crossref"
        },
        {
            "doi": "10.3390/info15080472",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1155/2021/6689337",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS55552.2023.10342500",
            "source": "crossref"
        },
        {
            "doi": "10.1117/1.OE.57.12.124105",
            "source": "crossref"
        },
        {
            "doi": "10.3390/s21041137",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JPROC.2014.2346153",
            "source": "crossref"
        },
        {
            "doi": "10.1088/1674-4926/42/1/013105",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00398",
            "source": "crossref"
        },
        {
            "doi": "10.3390/electronics7110304",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA.2015.7139657",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC19947.2020.9063149",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2011.6126544",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.ymssp.2020.106905",
            "source": "crossref"
        },
        {
            "doi": "10.1088/1361-6501/ad42bf",
            "source": "crossref"
        },
        {
            "doi": "10.1061/(ASCE)EM.1943-7889.0001449",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2021.3058423",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.patcog.2014.01.005",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TMC.2023.3335221",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2016.2647639",
            "source": "crossref"
        },
        {
            "doi": "10.3390/s22207963",
            "source": "crossref"
        },
        {
            "doi": "10.5194/isprs-archives-XLVIII-2-W7-2024-9-2024",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2018.2800793",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2021.3068942",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00437",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-031-20071-7_34",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR42600.2020.00364",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACV57701.2024.00237",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2024.3371487",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-031-82487-6_5",
            "source": "crossref"
        },
        {
            "title": "Neuromorphic Perception and Navigation for Mobile Robots: A Review",
            "source": "crossref"
        },
        {
            "title": "Event-based vision: A survey",
            "source": "crossref"
        },
        {
            "title": "An analysis framework for event-based sensor performance",
            "source": "crossref"
        },
        {
            "title": "Automated characterization techniques and testbeds for event-based cameras",
            "source": "crossref"
        },
        {
            "title": "A QVGA 143 dB dynamic range frame-free PWM image sensor with lossless pixel-level video compression and time-domain CDS",
            "source": "crossref"
        },
        {
            "title": "Retinomorphic event-based vision sensors: Bioinspired cameras with spiking output",
            "source": "crossref"
        },
        {
            "title": "Event-based high-speed low-latency fiducial marker tracking",
            "source": "crossref"
        },
        {
            "title": "Detection of binary square fiducial markers using an event camera",
            "source": "crossref"
        },
        {
            "title": "Automatic generation and detection of highly reliable fiducial markers under occlusion",
            "source": "crossref"
        },
        {
            "title": "EV-tach: A handheld rotational speed estimation system with event camera",
            "source": "crossref"
        },
        {
            "title": "Accurate angular velocity estimation with an event camera",
            "source": "crossref"
        },
        {
            "title": "Measuring Vibrations with Event Cameras",
            "source": "crossref"
        },
        {
            "title": "The multivehicle stereo event camera dataset: An event camera dataset for 3D perception",
            "source": "crossref"
        },
        {
            "title": "Dsec: A stereo event camera dataset for driving scenarios",
            "source": "crossref"
        },
        {
            "title": "Optimization of Event Camera Bias Settings for a Neuromorphic Driver Monitoring System",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

The JRC INVISIONS Neuromorphic Sensors Parametric Tests Dataset is composed of repeated dynamic sequences captured by different event cameras (neuromorphic) and eventually integrated with a frame-based camera. Tests are performed with an optical table, which provides a controlled environment for precise image acquisition and manipulation and repeated events. Data is collected with different camera configurations in three distinct scenarios: moving targets, mechanical vibrations, rotating fan. For moving targets, spatially and temporally synchronized frame-based data is provided, with keypoints and corners computed on the frames. For mechanical vibrations, a vibrating bar and ArUco markers are recorded; the ground truth frequency and the frame equivalent fiducial marker is provided. For the rotating fan, the ground truth frequency is provided.
