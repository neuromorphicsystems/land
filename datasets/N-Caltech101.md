---
{
    "name": "N-Caltech101",
    "aliases": [],
    "year": 2015,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "ATIS"
    ],
    "other_sensors": [],
    "category": "Object Detection, Classification, and Tracking",
    "subcategory": [
        "Monitor Conversion",
        "Classification Datasets"
    ],
    "task": "Character and Object Recognition",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": false,
        "has_biases": true,
        "distribution_methods": [
            "Google Drive"
        ],
        "file_formats": [
            "Binary"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Google Drive",
                "url": "https://drive.google.com/drive/folders/1sY91hL_iHnmfRXSTc058bfZ0GQcEC6St",
                "format": "Binary",
                "available": true
            },
            {
                "name": "Mendeley Data",
                "url": "https://data.mendeley.com/datasets/cy6cvx3ryv/1",
                "format": "Binary",
                "available": true
            },
            {
                "name": "Dropbox",
                "url": "https://www.dropbox.com/scl/fo/4cbdh0uizneotdht26f2z/AO3Gt9s5jboBLBR4NBvpIA0?rlkey=z5weoe8zmw99fgj0taq9yyks1&e=1&dl=0",
                "format": "Binary",
                "available": true
            },
            {
                "name": "OneDrive",
                "url": "https://onedrive.live.com/?authkey=%21AAr8lj4RRv4hRaQ&id=E6FE5BAD352FF5EA%2129252&cid=E6FE5BAD352FF5EA",
                "format": "Binary",
                "available": true
            }
        ],
        "size_gb": 3.72,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades",
        "doi": "10.3389/fnins.2015.00437",
        "authors": [
            "Garrick Orchard",
            "Ajinkya Jayawant",
            "Gregory K. Cohen",
            "Nitish Thakor"
        ],
        "abstract": "Creating datasets for Neuromorphic Vision is a challenging task. A lack of available recordings from Neuromorphic Vision sensors means that data must typically be recorded speci\ufb01cally for dataset creation rather than collecting and labeling existing data. The task is further complicated by a desire to simultaneously provide traditional frame-based recordings to allow for direct comparison with traditional Computer Vision algorithms. Here we propose a method for converting existing Computer Vision static image datasets into Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving the sensor rather than the scene or image is a more biologically realistic approach to sensing and eliminates timing artifacts introduced by monitor updates when simulating motion on a computer monitor. We present conversion of two popular image datasets (MNIST and Caltech101) which have played important roles in the development of Computer Vision, and we provide performance metrics on these datasets using spike-based recognition algorithms. This work contributes datasets for future use in the \ufb01eld, as well as results from spike-based algorithms against which future works can compare. Furthermore, by converting datasets already popular in Computer Vision, we enable more direct comparison with frame-based approaches.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 543,
            "updated": "2025-07-03T09:45:56.217007"
        },
        {
            "source": "scholar",
            "count": 993,
            "updated": "2025-07-03T09:45:55.879548"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/1507.07629"
        },
        {
            "type": "paper",
            "url": "https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2015.00437/full"
        },
        {
            "type": "project_page",
            "url": "https://www.garrickorchard.com/datasets/n-caltech101"
        }
    ],
    "full_name": "Neuromorphic Caltech101",
    "additional_metadata": {
        "stereo": false
    },
    "bibtex": {
        "year": 2015,
        "month": "nov",
        "author": "Orchard, Garrick and Jayawant, Ajinkya and Cohen, Gregory K. and Thakor, Nitish",
        "journal": "Frontiers in Neuroscience",
        "urldate": "2023-10-06",
        "language": "en",
        "doi": "10.3389/fnins.2015.00437",
        "url": "http://journal.frontiersin.org/Article/10.3389/fnins.2015.00437/abstract",
        "issn": "1662-453X",
        "volume": "9",
        "title": "Converting {Static} {Image} {Datasets} to {Spiking} {Neuromorphic} {Datasets} {Using} {Saccades}",
        "type": "article",
        "key": "orchard_converting_2015"
    },
    "referenced_papers": [
        {
            "doi": "10.1016/S0079-6123(06)54009-9",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2005.09.012",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2015.2389824",
            "source": "crossref"
        },
        {
            "doi": "10.1109/5.726791",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1371/journal.pcbi.0030031",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2013.00178",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2015.2392947",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2013.71",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JPROC.2014.2346153",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2012.2230553",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2007.56",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2015.00374",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2013.00153",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2014.2362542",
            "source": "crossref"
        },
        {
            "title": "Microsaccades: a microcosm for research on oculomotor control, attention, and visual perception",
            "source": "crossref"
        },
        {
            "title": "Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories",
            "source": "crossref"
        },
        {
            "title": "Spatial pyramid pooling in deep convolutional networks for visual recognition",
            "source": "crossref"
        },
        {
            "title": "Handling imbalanced datasets: a review",
            "source": "crossref"
        },
        {
            "title": "Gradient-based learning applied to document recognition",
            "source": "crossref"
        },
        {
            "title": "A 128x128 120 dB 15 us latency asynchronous temporal contrast vision sensor",
            "source": "crossref"
        },
        {
            "title": "Unsupervised learning of visual features through spike timing dependent plasticity",
            "source": "crossref"
        },
        {
            "title": "Real-time classification and sensor fusion with a spiking deep belief network",
            "source": "crossref"
        },
        {
            "title": "Hfirst: a temporal approach to object recognition",
            "source": "crossref"
        },
        {
            "title": "Mapping from frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing\u2013application to feedforward ConvNets",
            "source": "crossref"
        },
        {
            "title": "A QVGA 143 dB dynamic range frame-free PWM image sensor With lossless pixel-level video compression and time-domain CDS",
            "source": "crossref"
        },
        {
            "title": "Retinomorphic event-based vision sensors: bioinspired cameras with spiking output",
            "source": "crossref"
        },
        {
            "title": "A 128, 128 1.5% contrast sensitivity 0.9% FPN 3 \u03bcs latency 4 mW asynchronous frame-free dynamic vision sensor using transimpedance preamplifiers",
            "source": "crossref"
        },
        {
            "title": "Robust object recognition with cortex-like mechanisms",
            "source": "crossref"
        },
        {
            "title": "Benchmarking neuromorphic vision: lessons learnt from computer vision",
            "source": "crossref"
        },
        {
            "title": "Synthesis of neural networks for spatio-temporal spike pattern recognition and processing",
            "source": "crossref"
        },
        {
            "title": "The new data and new challenges in multimedia research",
            "source": "crossref"
        },
        {
            "title": "Unbiased look at dataset bias",
            "source": "crossref"
        },
        {
            "title": "Regularization of neural networks using dropconnect",
            "source": "crossref"
        },
        {
            "title": "Feedforward categorization on AER motion events using cortex-Like features in a spiking neural network",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

Each example is a separate binary file (e.g. 'accordion\\image_0001.bin') consisting of a list of events. Each event occupies 40 bits arranged as described below:

bit 39 - 32: Xaddress (in pixels)
bit 31 - 24: Yaddress (in pixels)
bit 23: Polarity (0 for OFF, 1 for ON)
bit 22 - 0: Timestamp (in microseconds)

The filenames and directory structure match the original Caltech101 dataset so that spike recordings inlcuded here can be backtraced to the original images.

A Matlab function for "Read_Ndataset.m" is provided for reading these binary files into Matlab, and a Matlab script "RunMe.m" shows how to read data in and use the provided Matlab functions

Additional Matlab functions are available at: http://www.garrickorchard.com/code

Bounding box and object contour annotations are provided for the data, each in a separate file (e.g. 'accordion\\annotation_0001.bin'). Each file contains two boundaries. The first is a rectangular box, while the second traces out the object countour. A Matlab script is provided showing how to read the data into Matlab.

For those wishing to write their own functions, the binary values are written as 16bit signed integers

word 0: dimension of points for box (2 because these are 2D images)
word 1: number of points in the box boundary ('N')
word 2: 1st dimension of box point 1
word 3: 2nd dimension of box point 1
word 4: 1st dimension of box point 2
word 5: 2nd dimension of box point 2
.
.
.
word N*2: 1st dimension of box point N
word N*2+1: 2nd dimension of box point N

word N*2+2: dimension of points for contour (2 because these are 2D images)
word N*2+3: number of points in the contour boundary ('M')
word N*2+4: 1st dimension of contour point 1
word N*2+5: 2nd dimension of contour point 1
word N*2+6: 1st dimension of contour point 2
word N*2+7: 2nd dimension of contour point 2
.
.
.
word (N+M+1)\*2: 1st dimension of contour point M
word (N+M+1)\*2+1: 2nd dimension of contour point M

The bias parameters used by the ATIS during recording are:
APSvrefL: 3050mV
APSvrefH: 3150mV
APSbiasOut: 750mV
APSbiasHyst: 620mV
CtrlbiasLP: 620mV
APSbiasTail: 700mV
CtrlbiasLBBuff: 950mV
TDbiasCas: 2000mV
CtrlbiasDelTD: 400mV
TDbiasDiffOff: 620mV
CtrlbiasSeqDelAPS: 320mV
TDbiasDiffOn: 780mV
CtrlbiasDelAPS: 350mV
TDbiasInv: 880mV
biasSendReqPdY: 850mV
TDbiasFo: 2950mV
biasSendReqPdX: 1150mV
TDbiasDiff: 700mV
CtrlbiasGB: 1050mV
TDbiasBulk: 2680mV
TDbiasReqPuY: 810mV
TDbiasRefr: 2900mV
TDbiasReqPuX: 1240mV
TDbiasPR: 3150mV
APSbiasReqPuY: 1100mV
APSbiasReqPuX: 820mV
