---
{
    "name": "evbench",
    "aliases": [],
    "year": 2016,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DAVIS240"
    ],
    "other_sensors": [
        "Kinect RGB-D camera"
    ],
    "category": "Visual Navigation",
    "subcategory": [
        "Visual Odometry",
        "Robotics"
    ],
    "task": "Visual Navigation",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": true,
        "has_biases": true,
        "distribution_methods": [
            "Direct Download"
        ],
        "file_formats": [
            "aedat"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Direct Download",
                "url": "https://atcproyectos.ugr.es/realtimeasoc/protected/evbench.html",
                "format": "aedat",
                "available": true
            }
        ],
        "size_gb": 0.813,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "A Dataset for Visual Navigation with Neuromorphic Methods",
        "doi": "10.3389/fnins.2016.00049",
        "authors": [
            "Francisco Barranco",
            "Cornelia Fermuller",
            "Yiannis Aloimonos",
            "Tobi Delbruck"
        ],
        "abstract": "Standardized benchmarks in Computer Vision have greatly contributed to the advance of approaches to many problems in the field. If we want to enhance the visibility of event-driven vision and increase its impact, we will need benchmarks that allow comparison among different neuromorphic methods as well as comparison to Computer Vision conventional approaches. We present datasets to evaluate the accuracy of frame-free and frame-based approaches for tasks of visual navigation. Similar to conventional Computer Vision datasets, we provide synthetic and real scenes, with the synthetic data created with graphics packages, and the real data recorded using a mobile robotic platform carrying a dynamic and active pixel vision sensor (DAVIS) and an RGB+Depth sensor. For both datasets the cameras move with a rigid motion in a static scene, and the data includes the images, events, optic flow, 3D camera motion, and the depth of the scene, along with calibration procedures. Finally, we also provide simulated event data generated synthetically from well-known frame-based optical flow datasets.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 33,
            "updated": "2025-07-14T07:56:55.524490"
        },
        {
            "source": "scholar",
            "count": 67,
            "updated": "2025-07-14T07:56:55.186040"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://www.frontiersin.org/articles/10.3389/fnins.2016.00049/full"
        },
        {
            "type": "project_page",
            "url": "https://atcproyectos.ugr.es/realtimeasoc/protected/evbench.html"
        },
        {
            "type": "github_page",
            "url": "https://github.com/fbarranco/eventVision-evbench"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "stereo": false
    },
    "bibtex": {
        "year": 2016,
        "month": "feb",
        "author": "Barranco, Francisco and Fermuller, Cornelia and Aloimonos, Yiannis and Delbruck, Tobi",
        "journal": "Frontiers in Neuroscience",
        "urldate": "2023-09-05",
        "language": "en",
        "doi": "10.3389/fnins.2016.00049",
        "url": "http://journal.frontiersin.org/Article/10.3389/fnins.2016.00049/abstract",
        "issn": "1662-453X",
        "volume": "10",
        "title": "A {Dataset} for {Visual} {Navigation} with {Neuromorphic} {Methods}",
        "type": "article",
        "key": "barranco_dataset_2016"
    },
    "referenced_papers": [
        {
            "doi": "10.1007/s11263-010-0390-2",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JPROC.2014.2347207",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-319-19258-1_27",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TVLSI.2011.2145423",
            "source": "crossref"
        },
        {
            "doi": "10.1007/BF01420984",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2013.2273537",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "doi": "10.1023/A:1007987001439",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA.2014.6906931",
            "source": "crossref"
        },
        {
            "doi": "10.1007/BF00056772",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2012.6248074",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-1-4471-4640-7_8",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1002/9781118927601",
            "source": "crossref"
        },
        {
            "doi": "10.1098/rspb.1980.0057",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2012.171",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISSCC.2005.1494019",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JPROC.2014.2346763",
            "source": "crossref"
        },
        {
            "doi": "10.1007/3-540-57956-7_5",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2013.71",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-642-03798-6_2",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2011.04.004",
            "source": "crossref"
        },
        {
            "doi": "10.1023/A:1014573219977",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS.2012.6385773",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s11263-013-0644-x",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS.2011.6095122",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-1-4612-4380-9_16",
            "source": "crossref"
        },
        {
            "title": "A database and evaluation methodology for optical flow",
            "source": "crossref"
        },
        {
            "title": "Contour motion estimation for asynchronous event-driven cameras",
            "source": "crossref"
        },
        {
            "title": "Bio-inspired motion estimation with event-driven sensors",
            "source": "crossref"
        },
        {
            "title": "Parallel architecture for hierarchical optical flow estimation based on fpga",
            "source": "crossref"
        },
        {
            "title": "Performance of optical flow techniques",
            "source": "crossref"
        },
        {
            "title": "Event-based visual flow",
            "source": "crossref"
        },
        {
            "title": "A 240\u00d7 180 130 db 3 \u03bcs latency global shutter spatiotemporal vision sensor",
            "source": "crossref"
        },
        {
            "title": "Improved accuracy in gradient-based optical flow estimation",
            "source": "crossref"
        },
        {
            "title": "A naturalistic open source movie for optical flow evaluation",
            "source": "crossref"
        },
        {
            "title": "Low-latency event-based visual odometry",
            "source": "crossref"
        },
        {
            "title": "Computation of component image velocity from local phase information",
            "source": "crossref"
        },
        {
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "source": "crossref"
        },
        {
            "title": "A category-level 3d object dataset: putting the kinect to work",
            "source": "crossref"
        },
        {
            "title": "A 128\u00d7 128 120 db 15 \u03bcs latency asynchronous temporal contrast vision sensor",
            "source": "crossref"
        },
        {
            "title": "The interpretation of a moving retinal image",
            "source": "crossref"
        },
        {
            "title": "Learning a confidence measure for optical flow",
            "source": "crossref"
        },
        {
            "title": "Temporal change threshold detection imager",
            "source": "crossref"
        },
        {
            "title": "Bioinspired visual motion estimation",
            "source": "crossref"
        },
        {
            "title": "Optical flow estimation: advances and comparisons",
            "source": "crossref"
        },
        {
            "title": "Mapping from frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing\u2013application to feedforward convnets",
            "source": "crossref"
        },
        {
            "title": "A qvga 143 db dynamic range frame-free pwm image sensor with lossless pixel-level video compression and time-domain cds",
            "source": "crossref"
        },
        {
            "title": "An efficient linear method for the estimation of ego-motion from optical flow",
            "source": "crossref"
        },
        {
            "title": "A review and evaluation of methods estimating ego-motion",
            "source": "crossref"
        },
        {
            "title": "On the spatial statistics of optical flow",
            "source": "crossref"
        },
        {
            "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms",
            "source": "crossref"
        },
        {
            "title": "A benchmark for the evaluation of RGB-D SLAM systems",
            "source": "crossref"
        },
        {
            "title": "A quantitative analysis of current practices in optical flow estimation and the principles behind them",
            "source": "crossref"
        },
        {
            "title": "Bio-inspired optic flow from event-based neuromorphic sensor input",
            "source": "crossref"
        },
        {
            "title": "Robust embedded egomotion estimation",
            "source": "crossref"
        },
        {
            "title": "Individual comparisons by ranking methods",
            "source": "crossref"
        }
    ]
}
---


### Dataset Structure