---
{
    "name": "Kanamaru2023",
    "aliases": [],
    "year": 2023,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DVXplorer"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "Face Detection",
        "Lip-reading"
    ],
    "task": "Face Detection and Lip-reading",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [],
        "file_formats": [],
        "availability_comment": "Dataset is available on request (privacy reasons)",
        "dataset_links": []
    },
    "paper": {
        "title": "Isolated single sound lip-reading using a frame-based camera and event-based camera",
        "doi": "10.3389/frai.2022.1070964",
        "authors": [
            "Tatsuya Kanamaru",
            "Taiki Arakane",
            "Takeshi Saitoh"
        ],
        "abstract": "Unlike the conventional frame-based camera, the event-based camera detects changes in the brightness value for each pixel over time. This research work on lip-reading as a new application by the event-based camera. This paper proposes an event camera-based lip-reading for isolated single sound recognition. The proposed method consists of imaging from event data, face and facial feature points detection, and recognition using a Temporal Convolutional Network. Furthermore, this paper proposes a method that combines the two modalities of the frame-based camera and an event-based camera. In order to evaluate the proposed method, the utterance scenes of 15 Japanese consonants from 20 speakers were collected using an event-based camera and a video camera and constructed an original dataset. Several experiments were conducted by generating images at multiple frame rates from an event-based camera. As a result, the highest recognition accuracy was obtained in the image of the event-based camera at 60 fps. Moreover, it was confirmed that combining two modalities yields higher recognition accuracy than a single modality.",
        "open_access": true
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 8,
            "updated": "2025-07-02T14:51:42.808591"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.1070964/full"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_subjects": "23",
        "num_males": "16",
        "num_females": "7",
        "stereo": false
    },
    "bibtex": {
        "pages": "1070964",
        "year": 2023,
        "month": "jan",
        "author": "Kanamaru, Tatsuya and Arakane, Taiki and Saitoh, Takeshi",
        "journal": "Frontiers in Artificial Intelligence",
        "urldate": "2024-08-16",
        "language": "en",
        "doi": "10.3389/frai.2022.1070964",
        "url": "https://www.frontiersin.org/articles/10.3389/frai.2022.1070964/full",
        "issn": "2624-8212",
        "volume": "5",
        "title": "Isolated single sound lip-reading using a frame-based camera and event-based camera",
        "type": "article",
        "key": "kanamaru_isolated_2023"
    },
    "referenced_papers": [
        {
            "doi": "10.21437/Interspeech.2018-1943",
            "source": "crossref"
        },
        {
            "doi": "10.48550/arXiv.1611.01599",
            "source": "crossref"
        },
        {
            "doi": "10.48550/arXiv.1803.01271",
            "source": "crossref"
        },
        {
            "doi": "10.1121/1.2229005",
            "source": "crossref"
        },
        {
            "doi": "10.1145/2671188.2749408",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ACCESS.2020.3036865",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2020.00587",
            "source": "crossref"
        },
        {
            "doi": "10.1109/WACVW50321.2020.9096944",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICPR.2018.8545664",
            "source": "crossref"
        },
        {
            "doi": "10.48550/arXiv.1409.1556",
            "source": "crossref"
        },
        {
            "title": "Deep lip reading: a comparison of models and an online application,",
            "source": "crossref"
        },
        {
            "title": "LipNet: end-to-end sentence-level lipreading",
            "source": "crossref"
        },
        {
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "source": "crossref"
        },
        {
            "title": "Lip reading sentences in the wild,",
            "source": "crossref"
        },
        {
            "title": "Lip reading in the wild,",
            "source": "crossref"
        },
        {
            "title": "An audio-visual corpus for speech perception and automatic speech recognition",
            "source": "crossref"
        },
        {
            "title": "Histograms of oriented gradients for human detection,",
            "source": "crossref"
        },
        {
            "title": "Retinaface: Single-shot multi-level face localisation in the wild,",
            "source": "crossref"
        },
        {
            "title": "Multi-view face detection using deep convolutional neural networks,",
            "source": "crossref"
        },
        {
            "title": "a survey of research on lipreading technology",
            "source": "crossref"
        },
        {
            "title": "Deep residual learning for image recognition,",
            "source": "crossref"
        },
        {
            "title": "One millisecond face alignment with an ensemble of regression trees,",
            "source": "crossref"
        },
        {
            "title": "Imagenet classification with deep convolutional neural networks,",
            "source": "crossref"
        },
        {
            "title": "Event-based face detection and tracking using the dynamics of eye blinks",
            "source": "crossref"
        },
        {
            "title": "Lip reading deep network exploiting multi-modal spiking visual and auditory sensors,",
            "source": "crossref"
        },
        {
            "title": "Lipreading using temporal convolutional networks,",
            "source": "crossref"
        },
        {
            "title": "3DCNN-based mouth shape recognition for patient with intractable neurological diseases,",
            "source": "crossref"
        },
        {
            "title": "Boosted kernelized correlation filters for event-based face detection,",
            "source": "crossref"
        },
        {
            "title": "A deep pyramid deformable part model for face detection,",
            "source": "crossref"
        },
        {
            "title": "SSSD: speech scene database by smart device for visual speech recognition,",
            "source": "crossref"
        },
        {
            "title": "Japanese sentence dataset for lip-reading,",
            "source": "crossref"
        },
        {
            "title": "Very deep convolutional networks for large-scale image recognition",
            "source": "crossref"
        },
        {
            "title": "Rapid object detection using a boosted cascade of simple features,",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

The dataset contains 23 , consisting of 16 and 7.
