---
{
    "name": "3ET+",
    "aliases": [],
    "year": 2024,
    "modality": [
        "Vision"
    ],
    "sensors": [
        "DVXplorer"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "subcategory": [
        "Eye Tracking",
        "Blink Detection"
    ],
    "task": "Eye and pupil tracking",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": true,
        "has_frames": false,
        "has_biases": false,
        "distribution_methods": [
            "Kaggle"
        ],
        "file_formats": [
            "HDF5"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Kaggle",
                "url": "https://www.kaggle.com/competitions/event-based-eye-tracking-ais2024",
                "format": "HDF5",
                "available": true
            }
        ],
        "size_gb": 9.2,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "Event-Based Eye Tracking. AIS 2024 Challenge Survey",
        "doi": "10.1109/CVPRW63382.2024.00590",
        "authors": [
            "Zuowen Wang",
            "Chang Gao",
            "Zongwei Wu",
            "Marcos V. Conde",
            "Radu Timofte",
            "Shih-Chii Liu",
            "Qinyu Chen",
            "Zheng-jun Zha",
            "Wei Zhai",
            "Han Han",
            "Bohao Liao",
            "Yuliang Wu",
            "Zengyu Wan",
            "Zhong Wang",
            "Yang Cao",
            "Ganchao Tan",
            "Jinze Chen",
            "Yan Ru Pei",
            "Sasskia Br\u00fcers",
            "S\u00e9bastien Crouzet",
            "Douglas McLelland",
            "Oliver Coenen",
            "Baoheng Zhang",
            "Yizhao Gao",
            "Jingyuan Li",
            "Hayden Kwok-Hay So",
            "Philippe Bich",
            "Chiara Boretti",
            "Luciano Prono",
            "Mircea Lic\u0103",
            "David Dinucu-Jianu",
            "C\u0103t\u0103lin Gr\u00eeu",
            "Xiaopeng Lin",
            "Hongwei Ren",
            "Bojun Cheng",
            "Xinan Zhang",
            "Valentin Vial",
            "Anthony Yezzi",
            "James Tsai"
        ],
        "abstract": "This survey reviews the AIS 2024 Event-Based Eye Tracking (EET) Challenge. The task of the challenge focuses on processing eye movement recorded with event cameras and predicting the pupil center of the eye. The challenge emphasizes efficient eye tracking with event cameras to achieve good task accuracy and efficiency trade-off. During the challenge period 38 participants registered for the Kaggle competition and 8 teams submitted a challenge factsheet. The novel and diverse methods from the submitted factsheets are reviewed and analyzed in this survey to advance future event-based eye tracking research.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 8,
            "updated": "2025-06-12T13:48:01.737643"
        },
        {
            "source": "scholar",
            "count": 25,
            "updated": "2025-06-12T13:48:01.547729"
        }
    ],
    "links": [
        {
            "type": "preprint",
            "url": "https://arxiv.org/abs/2404.11770"
        },
        {
            "type": "paper",
            "url": "https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/html/Wang_Event-Based_Eye_Tracking._AIS_2024_Challenge_Survey_CVPRW_2024_paper.html"
        },
        {
            "type": "project_page",
            "url": "https://eetchallenge.github.io/EET.github.io/"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_subjects": "13",
        "num_classes": "5",
        "stereo": false
    },
    "bibtex": {
        "pages": "5810--5825",
        "year": 2024,
        "month": "jun",
        "author": "Wang, Zuowen and Gao, Chang and Wu, Zongwei and Conde, Marcos V. and Timofte, Radu and Liu, Shih-Chii and Chen, Qinyu and Zha, Zheng-jun and Zhai, Wei and Han, Han and Liao, Bohao and Wu, Yuliang and Wan, Zengyu and Wang, Zhong and Cao, Yang and Tan, Ganchao and Chen, Jinze and Pei, Yan Ru and Br\u00fcers, Sasskia and Crouzet, S\u00e9bastien and McLelland, Douglas and Coenen, Oliver and Zhang, Baoheng and Gao, Yizhao and Li, Jingyuan and So, Hayden Kwok-Hay and Bich, Philippe and Boretti, Chiara and Prono, Luciano and Lic\u0103, Mircea and Dinucu-Jianu, David and Gr\u00eeu, C\u0103t\u0103lin and Lin, Xiaopeng and Ren, Hongwei and Cheng, Bojun and Zhang, Xinan and Vial, Valentin and Yezzi, Anthony and Tsai, James",
        "publisher": "IEEE",
        "booktitle": "2024 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})",
        "urldate": "2024-11-18",
        "language": "en",
        "doi": "10.1109/CVPRW63382.2024.00590",
        "url": "https://ieeexplore.ieee.org/document/10677920/",
        "isbn": "9798350365474",
        "copyright": "https://doi.org/10.15223/policy-029",
        "title": "Event-{Based} {Eye} {Tracking}. {AIS} 2024 {Challenge} {Survey}",
        "address": "Seattle, WA, USA",
        "type": "inproceedings",
        "key": "wang_event-based_2024-1"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/TNNLS.2018.2852335",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICIP46576.2022.9898061",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TVLSI.2020.2976454",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCAD.2022.3158834",
            "source": "crossref"
        },
        {
            "doi": "10.1109/BioCAS58349.2023.10389062",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00319",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00905",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW63382.2024.00592",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW63382.2024.00591",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3304109.3325818",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TVCG.2023.3247058",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISMAR52148.2021.00053",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TNNLS.2022.3180209",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52729.2023.01334",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00140",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1177/000486740003401S14",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2015.7168734",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58598-3_25",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW63382.2024.00587",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fneur.2017.00592",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52733.2024.01715",
            "source": "crossref"
        },
        {
            "doi": "10.1145/3407197.3407211",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW56347.2022.00301",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00401",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-031-20053-3_27",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICIP46576.2022.9897432",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW63382.2024.00590",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52733.2024.01812",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA48891.2023.10161563",
            "source": "crossref"
        },
        {
            "title": "MLflow: A Machine Learning Lifecycle Platform",
            "source": "crossref"
        },
        {
            "title": "DVXplorer Mini User Guide",
            "source": "crossref"
        },
        {
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "source": "crossref"
        },
        {
            "title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "source": "crossref"
        },
        {
            "title": "Adam: A method for stochastic optimization",
            "source": "crossref"
        },
        {
            "title": "Tonic: event-based datasets and transformations",
            "source": "crossref"
        },
        {
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "source": "crossref"
        },
        {
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "source": "crossref"
        },
        {
            "title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
            "source": "crossref"
        },
        {
            "title": "Dsec-mos: Segment any moving object with moving ego vehicle",
            "source": "crossref"
        },
        {
            "title": "Event-free moving object segmentation from moving ego vehicle",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

- Contains data from 13 subjects
- Contains 5 different types of activities
- Ground truth is labelled at 100 Hz

### Datasets Referenced in this Paper

\[[3ET]\]
\[[3ET+]\]
