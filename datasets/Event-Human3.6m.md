---
{
    "name": "Event-Human3.6m",
    "aliases": [],
    "year": 2023,
    "modalities": [
        "Vision"
    ],
    "sensors": [
        "V2E"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Human Pose Estimation"
    ],
    "description": "Human Pose Estimation Dataset",
    "dataset_properties": {
        "available_online": true,
        "has_real_data": false,
        "has_simulated_data": true,
        "has_ground_truth": true,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [
            "Zenodo"
        ],
        "file_formats": [
            "HDF5"
        ],
        "availability_comment": "",
        "dataset_links": [
            {
                "name": "Zenodo",
                "url": "https://zenodo.org/records/7842598",
                "format": "HDF5",
                "doi": "10.5281/zenodo.7842598",
                "available": true
            }
        ],
        "size_gb": 39.0,
        "size_type": "Compressed"
    },
    "paper": {
        "title": "MoveEnet: Online High-Frequency Human Pose Estimation with an Event Camera",
        "doi": "10.1109/CVPRW59228.2023.00420",
        "authors": [
            "Gaurvi Goyal",
            "Franco Di Pietro",
            "Nicolo Carissimi",
            "Arren Glover",
            "Chiara Bartolozzi"
        ],
        "abstract": "Human Pose Estimation (HPE) is crucial as a building block for tasks that are based on the accurate understanding of human position, pose and movements. Therefore, accuracy and efficiency in this block echo throughout a system, making it important to find efficient methods, that run at fast rates for online applications. The state of the art for mainstream sensors has made considerable advances, but event camera based HPE is still in its infancy. Event cameras boast high rates of data capture in a compact data structure, with advantages like high dynamic range and low power consumption. In this work, we present a system for a high frequency estimation of 2D, single-person Human Pose with event cameras. We provide an online system, that can be paired directly with an event camera to obtain high accuracy in real time. For quantitative results, we present our results on two large scale datasets, DHP19 and event-Human 3.6m. The system is robust to variance in the resolution of the camera and can run at up to 100Hz and an accuracy 89%.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 22,
            "updated": "2025-09-05T13:42:41.837546"
        },
        {
            "source": "scholar",
            "count": 35,
            "updated": "2025-09-05T13:42:42.488701"
        }
    ],
    "links": [
        {
            "type": "github_page",
            "url": "https://github.com/event-driven-robotics/hpe-core"
        },
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/10208530"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "num_subjects": "11",
        "num_classes": "17",
        "sensor_resolution": "640x480"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/EBCCSP56922.2022.9845622",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2018.00186",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2020.3008413",
            "source": "crossref"
        },
        {
            "doi": "10.1109/LRA.2019.2893427",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICEIEC.2019.8784591",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00582",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58565-5_3",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR52688.2022.01205",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV.2019.00667",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW53098.2021.00150",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2014.471",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR42600.2020.00502",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.imavis.2022.104403",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2021.103225",
            "source": "crossref"
        },
        {
            "doi": "10.1109/IROS.2018.8594119",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW53098.2021.00144",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.01049",
            "source": "crossref"
        },
        {
            "doi": "10.1145/2816795.2818013",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2013.248",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2019.00108",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.autcon.2018.05.033",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TPAMI.2016.2574707",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-01252-6_26",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICCV48922.2021.01081",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.displa.2022.102225",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICRA.2015.7139876",
            "source": "crossref"
        },
        {
            "doi": "10.1007/978-3-030-58529-7_29",
            "source": "crossref"
        },
        {
            "doi": "10.1016/j.cviu.2019.102897",
            "source": "crossref"
        },
        {
            "doi": "10.1109/3DV57658.2022.00023",
            "source": "crossref"
        },
        {
            "doi": "10.26599/TST.2018.9010100",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPRW.2019.00217",
            "source": "crossref"
        },
        {
            "doi": "10.1109/EBCCSP56922.2022.9845526",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.143",
            "source": "crossref"
        },
        {
            "title": "luvHarris: A Practical Corner Detector for Event-cameras",
            "source": "crossref"
        },
        {
            "title": "Asynchronous, photometric feature tracking using events and frames",
            "source": "crossref"
        },
        {
            "title": "Real-time human pose estimation using rgbd images and deep learning",
            "source": "crossref"
        },
        {
            "title": "Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time",
            "source": "crossref"
        },
        {
            "title": "Movenet: Ultra fast and accurate pose detection model",
            "source": "crossref"
        },
        {
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "source": "crossref"
        },
        {
            "title": "Recent advances of monocular 2d and 3d human pose estimation: A deep learning perspective",
            "source": "crossref"
        },
        {
            "title": "Microsoft coco: Common objects in context",
            "source": "crossref"
        },
        {
            "title": "Openpose: Realtime multi-person 2d pose estimation using part affinity fields",
            "source": "crossref"
        },
        {
            "title": "How transferable are features in deep neural networks?",
            "source": "crossref"
        }
    ],
    "bibtex": {
        "pages": "4024\u20134033",
        "month": "jun",
        "year": 2023,
        "author": "Goyal, Gaurvi and Di Pietro, Franco and Carissimi, Nicolo and Glover, Arren and Bartolozzi, Chiara",
        "publisher": "IEEE",
        "booktitle": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "doi": "10.1109/cvprw59228.2023.00420",
        "url": "http://dx.doi.org/10.1109/CVPRW59228.2023.00420",
        "title": "MoveEnet: Online High-Frequency Human Pose Estimation with an Event Camera",
        "type": "inproceedings",
        "key": "Goyal_2023"
    }
}
---

# Dataset Description

The Human 3.6m dataset is a widely used large scale, multi-view, benchmark video dataset for single person 2D HPE. Each sequence is captured from 4 video cameras (resolution 1000×1000 pixels; frequency 50 Hz), 2 facing the front and 2 behind each subject. It was recorded on 11 subjects and contains 17 scenarios. We converted this dataset to events with the resolution of the target camera: 640×480, employing v2e, a state of the art method. v2e first generates a set of intermediate frames with a slow-motion model, then creates the events based on the changes between these new consecutive frames. It also adds synthetic noise to the output with a noise model derived from the event camera. The dataset provides a pose annotation frequency of 50 Hz.