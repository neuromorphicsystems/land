---
{
    "name": "Li2019b",
    "aliases": [],
    "year": 2019,
    "modalities": [
        "Audio",
        "Vision"
    ],
    "sensors": [
        "DAS",
        "DAVIS240"
    ],
    "other_sensors": [],
    "category": "Human-centric Recordings",
    "tags": [
        "Lip-reading",
        "Monitor Conversion"
    ],
    "description": "Lip Reading",
    "dataset_properties": {
        "available_online": false,
        "has_real_data": true,
        "has_simulated_data": false,
        "has_ground_truth": false,
        "has_frames": true,
        "has_biases": false,
        "distribution_methods": [
            "None"
        ],
        "file_formats": [],
        "availability_comment": "",
        "dataset_links": []
    },
    "paper": {
        "title": "Lip Reading Deep Network Exploiting Multi-Modal Spiking Visual and Auditory Sensors",
        "doi": "10.1109/ISCAS.2019.8702565",
        "authors": [
            "Xiaoya Li",
            "Daniel Neil",
            "Tobi Delbruck",
            "Shih-Chii Liu"
        ],
        "abstract": "This work presents a lip reading deep neural network that fuses the asynchronous spiking outputs of two bio-inspired silicon multimodal sensors: the Dynamic Vision Sensor (DVS) and the Dynamic Audio Sensor (DAS). The fusion network is tested on the GRID visual-audio lipreading dataset. Classification is carried out using event-based features generated from the spikes of the DVS and DAS. Networks are trained separately on the two modalities and also jointly trained on both modalities. The jointly trained network when tested on DVS spike frames alone, showed a relative increase in accuracy of around 23% over that of the single DVS modality network.",
        "open_access": false
    },
    "citation_counts": [
        {
            "source": "crossref",
            "count": 15,
            "updated": "2025-07-07T09:02:45.010413"
        },
        {
            "source": "scholar",
            "count": 27,
            "updated": "2025-07-07T09:02:44.677611"
        }
    ],
    "links": [
        {
            "type": "paper",
            "url": "https://ieeexplore.ieee.org/document/8702565"
        }
    ],
    "full_name": "",
    "additional_metadata": {
        "source_dataset": "GRID",
        "num_male": "18",
        "num_female": "16",
        "unique_words": "51",
        "num_recordings": "24"
    },
    "bibtex": {
        "pages": "1--5",
        "year": 2019,
        "month": "may",
        "author": "Li, Xiaoya and Neil, Daniel and Delbruck, Tobi and Liu, Shih-Chii",
        "publisher": "IEEE",
        "booktitle": "2019 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})",
        "urldate": "2024-08-16",
        "language": "en",
        "doi": "10.1109/ISCAS.2019.8702565",
        "url": "https://ieeexplore.ieee.org/document/8702565/",
        "isbn": "978-1-72810-397-6",
        "copyright": "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
        "title": "Lip {Reading} {Deep} {Network} {Exploiting} {Multi}-{Modal} {Spiking} {Visual} and {Auditory} {Sensors}",
        "address": "Sapporo, Japan",
        "type": "inproceedings",
        "key": "li_lip_2019"
    },
    "referenced_papers": [
        {
            "doi": "10.1109/TPAMI.2013.71",
            "source": "crossref"
        },
        {
            "doi": "10.1162/neco.1997.9.8.1735",
            "source": "crossref"
        },
        {
            "doi": "10.3115/v1/D14-1179",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.781",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TC.2016.2630683",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2016.7539039",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2013.00178",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ISCAS.2016.7539099",
            "source": "crossref"
        },
        {
            "doi": "10.1109/FG.2018.00055",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2010.2085952",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2015.2425886",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2016.2604285",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2014.2342715",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TCSI.2006.887979",
            "source": "crossref"
        },
        {
            "doi": "10.1109/TBCAS.2013.2281834",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2012.2230553",
            "source": "crossref"
        },
        {
            "doi": "10.1109/EBCCSP.2016.7605233",
            "source": "crossref"
        },
        {
            "doi": "10.1109/JSSC.2007.914337",
            "source": "crossref"
        },
        {
            "doi": "10.1007/s10489-014-0629-7",
            "source": "crossref"
        },
        {
            "doi": "10.1109/CVPR.2017.367",
            "source": "crossref"
        },
        {
            "doi": "10.1109/ICASSP.2016.7472852",
            "source": "crossref"
        },
        {
            "doi": "10.1121/1.2229005",
            "source": "crossref"
        },
        {
            "doi": "10.3389/fnins.2018.00023",
            "source": "crossref"
        },
        {
            "title": "Multimodal deep learning",
            "source": "crossref"
        },
        {
            "title": "AER EAR: A matched silicon cochlea pair with address event representation interface",
            "source": "crossref"
        },
        {
            "title": "Asynchronous binaural spatial audition sensor with 2 &#x00D7; 64 &#x00D7; 4 channel output",
            "source": "crossref"
        },
        {
            "title": "OpenCV",
            "source": "crossref"
        }
    ]
}
---

### Dataset Structure

The dataset was based on the GRID audio-visual sentence corpus [^1]. It consists of 1000 sentences spoken by 34 speakers (18 male and 16 female ). Each sentence consists of six words with a specific structure. In total, the dataset contains 51 unique words.

Due to technical and quality reasons, only 24 of the GRID recordings were used in the final dataset.

The dataset was captured by displaying the GRID video files on a monitor, recorded by a DAVIS240C camera, and with the audio played played by the computer into the DAS.

\[^1\]: M. Cooke, J. Barker, S. Cunningham and X. Shao, "An audio-visual corpus for speech perception and automatic speech recognition",Â _The Journal of the Acoustical Society of America_, vol. 120, no. 5, 2006.
